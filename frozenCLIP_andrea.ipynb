{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXxIwh__s7c5"
      },
      "source": [
        "# Weakly supervised semantic segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3NbxPwBs7c-"
      },
      "source": [
        "## Multi stage training\n",
        "= use several individual models to form a training pipeline\n",
        "\n",
        "CAM $\\implies$ offline pixel-level pseudo-labeling $\\implies$ train segmentation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE8mhMM4s7c_"
      },
      "source": [
        "# Approach\n",
        "Past methods: Use CLIP to improve CAM for better pseudo-labeling\\\n",
        "**Ours**: CLIP-based single-stave pipeline for WSSS with CLIP used to extract strong semantic features for segmentation prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7DIUA2s7c_"
      },
      "source": [
        "# Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Colab and Train"
      ],
      "metadata": {
        "id": "t9sWMqVeVbdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Working in Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Working in Local\")\n",
        "\n",
        "if IN_COLAB:\n",
        "\n",
        "    from google.colab import userdata\n",
        "    import os\n",
        "\n",
        "    # Download FrozenCLIP repo\n",
        "    !git clone https://github.com/zbf1991/WeCLIP.git\n",
        "\n",
        "    # Download dataset\n",
        "    !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar\n",
        "    !tar -xf VOCdevkit_18-May-2011.tar\n",
        "    !rm VOCdevkit_18-May-2011.tar\n",
        "    !cd VOCdevkit/\n",
        "    !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "    !tar -xf VOCtrainval_11-May-2012.tar\n",
        "    !rm VOCtrainval_11-May-2012.tar\n",
        "    !cd ../\n",
        "\n",
        "    %cd /content/VOCdevkit/VOC2012/\n",
        "    !wget https://www.dropbox.com/scl/fi/xccys1fus0utdioi7nj4d/SegmentationClassAug.zip?rlkey=0wl8iz6sc40b3qf6nidun4rez&e=1&dl=0\n",
        "    !unzip SegmentationClassAug.zip?rlkey=0wl8iz6sc40b3qf6nidun4rez&dl=0\n",
        "\n",
        "    ## Download pretrained ViT-B (non necessario, lo scarica lo script)\n",
        "    # !mkdir WeCLIP/pretrained\n",
        "    # !wget https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\n",
        "    # !mv ViT-B-16.pt WeCLIP/pretrained/ViT-B-16.pt\n",
        "\n",
        "    # Function to modify config file\n",
        "    def modify_config(config_file, dataset_path, name_list_path, pretrain_path):\n",
        "        with open(config_file, 'r') as f:\n",
        "            config = f.read()\n",
        "\n",
        "        # Replace paths\n",
        "        config = config.replace(\"your/path/VOCdevkit/VOC2012\", dataset_path)\n",
        "        config = config.replace(\"your/path/WeCLIP/datasets/voc\", name_list_path)\n",
        "        config = config.replace(\"your/path/WeCLIP/pretrained/ViT-B-16.pt\", pretrain_path)\n",
        "\n",
        "        with open(config_file, 'w') as f:\n",
        "            f.write(config)\n",
        "\n",
        "    # Modify VOC config\n",
        "    voc_config = {\n",
        "        'dataset_path': '../VOCdevkit/VOC2012',\n",
        "        'name_list_path': 'datasets/voc',\n",
        "        'pretrain_path': 'ViT-B/16'\n",
        "    }\n",
        "    modify_config('WeCLIP/configs/voc_attn_reg.yaml', **voc_config)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXA9w4JivKoz",
        "outputId": "11239a4d-0612-44b0-d1e6-b3d9064a2cd7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working in Colab\n",
            "Cloning into 'WeCLIP'...\n",
            "remote: Enumerating objects: 147, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 147 (delta 37), reused 131 (delta 32), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (147/147), 5.51 MiB | 14.76 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "--2024-11-01 00:49:49--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 511488 (500K) [application/x-tar]\n",
            "Saving to: ‘VOCdevkit_18-May-2011.tar’\n",
            "\n",
            "VOCdevkit_18-May-20 100%[===================>] 499.50K   647KB/s    in 0.8s    \n",
            "\n",
            "2024-11-01 00:49:51 (647 KB/s) - ‘VOCdevkit_18-May-2011.tar’ saved [511488/511488]\n",
            "\n",
            "--2024-11-01 00:49:51--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1999639040 (1.9G) [application/x-tar]\n",
            "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
            "\n",
            "VOCtrainval_11-May- 100%[===================>]   1.86G  19.6MB/s    in 97s     \n",
            "\n",
            "2024-11-01 00:51:29 (19.6 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
            "\n",
            "--2024-11-01 00:51:39--  https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.253.69, 2620:1ec:29:1::69\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.253.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 350837078 (335M) [application/octet-stream]\n",
            "Saving to: ‘ViT-B-16.pt’\n",
            "\n",
            "ViT-B-16.pt         100%[===================>] 334.58M  9.15MB/s    in 39s     \n",
            "\n",
            "2024-11-01 00:52:19 (8.63 MB/s) - ‘ViT-B-16.pt’ saved [350837078/350837078]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pip<24.1\"\n",
        "!git clone https://github.com/zbf1991/WeCLIP.git\n",
        "\n",
        "# pydensecrf\n",
        "!pip install -U cython\n",
        "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
        "\n",
        "# mmcv\n",
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install openmim\n",
        "!mim install mmengine\n",
        "!mim install mmcv==2.1.0\n",
        "\n",
        "# ftfy and other dependencies\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install ftfy==6.1.1  # Specific version if needed\n",
        "\n",
        "# 2. Install omegaconf (from requirements.txt)\n",
        "# !pip install omegaconf==2.0.6\n",
        "\n",
        "!python -c \"import ftfy; import clip; print('Imports successful')\"\n",
        "\n",
        "!cat WeCLIP/requirements.txt | grep -v pydensecrf | grep -v mmcv | pip install -r /dev/stdin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oRR9a8Z15M2o",
        "outputId": "36802152-6a35-4536-ec2c-5d7653a269c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip<24.1\n",
            "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.0\n",
            "fatal: destination path 'WeCLIP' already exists and is not an empty directory.\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
            "Collecting git+https://github.com/lucasb-eyer/pydensecrf.git\n",
            "  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-t2jc9mlw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-req-build-t2jc9mlw\n",
            "  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 2723c7fa4f2ead16ae1ce3d8afe977724bb8f87f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pydensecrf\n",
            "  Building wheel for pydensecrf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydensecrf: filename=pydensecrf-1.0-cp310-cp310-linux_x86_64.whl size=3405189 sha256=b4a99c17e3759044e5aa2ed65f2ee79d515a18c9eff334b77d791233d1eab4fd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xs4ymvux/wheels/01/5b/61/87443ed3bf03dd2940375cf2f8b6fba88efece935465e490b0\n",
            "Successfully built pydensecrf\n",
            "Installing collected packages: pydensecrf\n",
            "Successfully installed pydensecrf-1.0\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m999.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2024.10.0)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.0+cu121\n",
            "    Uninstalling torchvision-0.20.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.0+cu121\n",
            "    Uninstalling torchaudio-2.5.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.0+cu121\n",
            "Successfully installed torch-2.1.0+cu121 torchaudio-2.1.0+cu121 torchvision-0.16.0+cu121 triton-2.1.0\n",
            "Collecting openmim\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.7)\n",
            "Collecting colorama (from openmim)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting model-index (from openmim)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting opendatalab (from openmim)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (2.2.2)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.9.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.7)\n",
            "Collecting ordered-set (from model-index->openmim)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pycryptodome (from opendatalab->openmim)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (4.66.6)\n",
            "Collecting openxlab (from opendatalab->openmim)\n",
            "  Downloading openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.16.0)\n",
            "Collecting filelock~=3.14.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (24.1)\n",
            "Collecting pytz>=2020.1 (from pandas->openmim)\n",
            "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting requests (from openmim)\n",
            "  Downloading requests-2.28.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting rich (from openmim)\n",
            "  Downloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting setuptools~=60.2.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading setuptools-60.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tqdm (from opendatalab->openmim)\n",
            "  Downloading tqdm-4.65.2-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests->openmim)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod>=1.7 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.22)\n",
            "Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Downloading openxlab-0.1.2-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2023.4-py2.py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.5/506.5 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.65.2-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
            "Downloading setuptools-60.2.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.1/953.1 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.17.0-py3-none-any.whl size=112372 sha256=3ed77abae6ae7d4b9c97f338b32b324c43e87147d5b3a1fadeebf3ddca2df772\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/04/7b/7e61b8157fdf211c5131375240d0d86ca82e2a88ead9672c88\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535316 sha256=83ebc2fe7311b4364aed1f377a0b5ee4854c661380596964311318b0f196761e\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/11/5e/08e7cb4e03a3e83b4862edd12d1143c8d3936a3dd57a3ee46d\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31407 sha256=e5451e732a83a23662be5c62c404144d586b225c51f647ed592954646ebc01d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "Successfully built oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: pytz, crcmod, urllib3, tqdm, setuptools, pycryptodome, ordered-set, jmespath, filelock, colorama, rich, requests, model-index, aliyun-python-sdk-core, aliyun-python-sdk-kms, oss2, openxlab, opendatalab, openmim\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2024.2\n",
            "    Uninstalling pytz-2024.2:\n",
            "      Successfully uninstalled pytz-2024.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.6\n",
            "    Uninstalling tqdm-4.66.6:\n",
            "      Successfully uninstalled tqdm-4.66.6\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.16.1\n",
            "    Uninstalling filelock-3.16.1:\n",
            "      Successfully uninstalled filelock-3.16.1\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.3\n",
            "    Uninstalling rich-13.9.3:\n",
            "      Successfully uninstalled rich-13.9.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.28.2 which is incompatible.\n",
            "pymc 5.17.0 requires rich>=13.7.1, but you have rich 13.4.2 which is incompatible.\n",
            "pytensor 2.25.5 requires filelock>=3.15, but you have filelock 3.14.0 which is incompatible.\n",
            "yfinance 0.2.48 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 colorama-0.4.6 crcmod-1.7 filelock-3.14.0 jmespath-0.10.0 model-index-0.1.11 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.1.2 ordered-set-4.1.0 oss2-2.17.0 pycryptodome-3.21.0 pytz-2023.4 requests-2.28.2 rich-13.4.2 setuptools-60.2.0 tqdm-4.65.2 urllib3-1.26.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              },
              "id": "5d529fc334e74f31a6fa9acf9e4acd15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu121/torch2.1.0/index.html\n",
            "Collecting mmengine\n",
            "  Downloading mmengine-0.10.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting addict (from mmengine)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmengine) (1.26.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmengine) (6.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine) (2.5.0)\n",
            "Collecting yapf (from mmengine)\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmengine) (4.10.0.84)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine) (2.18.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (8.5.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (2.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmengine) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine) (1.16.0)\n",
            "Downloading mmengine-0.10.5-py3-none-any.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.3/452.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, yapf, mmengine\n",
            "Successfully installed addict-2.4.0 mmengine-0.10.5 yapf-0.40.2\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu121/torch2.1.0/index.html\n",
            "Collecting mmcv==2.1.0\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu121/torch2.1.0/mmcv-2.1.0-cp310-cp310-manylinux1_x86_64.whl (94.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.1/94.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (2.4.0)\n",
            "Requirement already satisfied: mmengine>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (0.10.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (24.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (10.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (6.0.2)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (0.40.2)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv==2.1.0) (3.8.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv==2.1.0) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv==2.1.0) (2.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==2.1.0) (8.5.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==2.1.0) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==2.1.0) (2.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmcv==2.1.0) (3.20.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv==2.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv==2.1.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.3.0->mmcv==2.1.0) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (1.16.0)\n",
            "Installing collected packages: mmcv\n",
            "Successfully installed mmcv-2.1.0\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting ftfy==6.1.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.1.1) (0.2.13)\n",
            "Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "  Attempting uninstall: ftfy\n",
            "    Found existing installation: ftfy 6.3.1\n",
            "    Uninstalling ftfy-6.3.1:\n",
            "      Successfully uninstalled ftfy-6.3.1\n",
            "Successfully installed ftfy-6.1.1\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ModuleNotFoundError: No module named 'clip'\n",
            "Collecting imageio==2.22.4 (from -r /dev/stdin (line 1))\n",
            "  Downloading imageio-2.22.4-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting joblib==1.2.0 (from -r /dev/stdin (line 2))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting lxml==4.9.2 (from -r /dev/stdin (line 3))\n",
            "  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting matplotlib==3.6.2 (from -r /dev/stdin (line 4))\n",
            "  Downloading matplotlib-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting mmsegmentation==1.0.0 (from -r /dev/stdin (line 5))\n",
            "  Downloading mmsegmentation-1.0.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting numpy==1.23.5 (from -r /dev/stdin (line 6))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting omegaconf==2.0.6 (from -r /dev/stdin (line 7))\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting opencv_python==4.6.0.66 (from -r /dev/stdin (line 8))\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting Pillow==9.3.0 (from -r /dev/stdin (line 9))\n",
            "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting regex==2023.3.23 (from -r /dev/stdin (line 10))\n",
            "  Downloading regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit_learn==1.2.0 (from -r /dev/stdin (line 11))\n",
            "  Downloading scikit_learn-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting setuptools==58.0.4 (from -r /dev/stdin (line 12))\n",
            "  Downloading setuptools-58.0.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting timm==0.6.12 (from -r /dev/stdin (line 13))\n",
            "  Downloading timm-0.6.12-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting tqdm==4.62.3 (from -r /dev/stdin (line 14))\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ttach==0.0.3 (from -r /dev/stdin (line 15))\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.2->-r /dev/stdin (line 4)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.2->-r /dev/stdin (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.2->-r /dev/stdin (line 4)) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.2->-r /dev/stdin (line 4)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.2->-r /dev/stdin (line 4)) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.2->-r /dev/stdin (line 4)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.2->-r /dev/stdin (line 4)) (2.8.2)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from mmsegmentation==1.0.0->-r /dev/stdin (line 5)) (3.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mmsegmentation==1.0.0->-r /dev/stdin (line 5)) (1.13.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6->-r /dev/stdin (line 7)) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6->-r /dev/stdin (line 7)) (4.12.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.0->-r /dev/stdin (line 11)) (3.5.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm==0.6.12->-r /dev/stdin (line 13)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.6.12->-r /dev/stdin (line 13)) (0.16.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.6.12->-r /dev/stdin (line 13)) (0.24.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.6.2->-r /dev/stdin (line 4)) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.12->-r /dev/stdin (line 13)) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.12->-r /dev/stdin (line 13)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.12->-r /dev/stdin (line 13)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.12->-r /dev/stdin (line 13)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.12->-r /dev/stdin (line 13)) (2024.10.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.12->-r /dev/stdin (line 13)) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.6.12->-r /dev/stdin (line 13)) (2.28.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->mmsegmentation==1.0.0->-r /dev/stdin (line 5)) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm==0.6.12->-r /dev/stdin (line 13)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.6.12->-r /dev/stdin (line 13)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.6.12->-r /dev/stdin (line 13)) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.6.12->-r /dev/stdin (line 13)) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.6.12->-r /dev/stdin (line 13)) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm==0.6.12->-r /dev/stdin (line 13)) (1.3.0)\n",
            "Downloading imageio-2.22.4-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmsegmentation-1.0.0-py3-none-any.whl (903 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m903.4/903.4 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.6/769.6 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-58.0.4-py3-none-any.whl (816 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.5/816.5 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: ttach, tqdm, setuptools, regex, Pillow, omegaconf, numpy, lxml, joblib, opencv_python, imageio, scikit_learn, matplotlib, timm, mmsegmentation\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.2\n",
            "    Uninstalling tqdm-4.65.2:\n",
            "      Successfully uninstalled tqdm-4.65.2\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 60.2.0\n",
            "    Uninstalling setuptools-60.2.0:\n",
            "      Successfully uninstalled setuptools-60.2.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.9.11\n",
            "    Uninstalling regex-2024.9.11:\n",
            "      Successfully uninstalled regex-2024.9.11\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 10.4.0\n",
            "    Uninstalling pillow-10.4.0:\n",
            "      Successfully uninstalled pillow-10.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.3.0\n",
            "    Uninstalling lxml-5.3.0:\n",
            "      Successfully uninstalled lxml-5.3.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: opencv_python\n",
            "    Found existing installation: opencv-python 4.10.0.84\n",
            "    Uninstalling opencv-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-python-4.10.0.84\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.36.0\n",
            "    Uninstalling imageio-2.36.0:\n",
            "      Successfully uninstalled imageio-2.36.0\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.11\n",
            "    Uninstalling timm-1.0.11:\n",
            "      Successfully uninstalled timm-1.0.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "arviz 0.20.0 requires setuptools>=60.0.0, but you have setuptools 58.0.4 which is incompatible.\n",
            "bigframes 1.24.0 requires matplotlib>=3.7.1, but you have matplotlib 3.6.2 which is incompatible.\n",
            "bigframes 1.24.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.24.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.2.0 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "dopamine-rl 4.0.9 requires tqdm>=4.64.1, but you have tqdm 4.62.3 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "openxlab 0.1.2 requires setuptools~=60.2.0, but you have setuptools 58.0.4 which is incompatible.\n",
            "openxlab 0.1.2 requires tqdm~=4.65.0, but you have tqdm 4.62.3 which is incompatible.\n",
            "plotnine 0.14.0 requires matplotlib>=3.8.0, but you have matplotlib 3.6.2 which is incompatible.\n",
            "pymc 5.17.0 requires rich>=13.7.1, but you have rich 13.4.2 which is incompatible.\n",
            "pytensor 2.25.5 requires filelock>=3.15, but you have filelock 3.14.0 which is incompatible.\n",
            "pytensor 2.25.5 requires setuptools>=59.0.0, but you have setuptools 58.0.4 which is incompatible.\n",
            "scikit-image 0.24.0 requires imageio>=2.33, but you have imageio 2.22.4 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "yfinance 0.2.48 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.3.0 imageio-2.22.4 joblib-1.2.0 lxml-4.9.2 matplotlib-3.6.2 mmsegmentation-1.0.0 numpy-1.23.5 omegaconf-2.0.6 opencv_python-4.6.0.66 regex-2023.3.23 scikit_learn-1.2.0 setuptools-58.0.4 timm-0.6.12 tqdm-4.62.3 ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/WeCLIP\n",
        "!python scripts/dist_clip_voc.py --config configs/voc_attn_reg.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcVSAne44ITL",
        "outputId": "3cbea924-e477-4f0b-bd3a-70f2c59d02c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/WeCLIP\n",
            "2024-11-01 01:23:12.966212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-01 01:23:12.986339: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-01 01:23:12.992368: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-01 01:23:13.007075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-01 01:23:14.252963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-11-01 01:23:17,925 - dist_clip_voc.py - INFO: \n",
            "args: Namespace(config='configs/voc_attn_reg.yaml', seg_detach=False, work_dir=None, radius=8, crop_size=320)\n",
            "2024-11-01 01:23:17,925 - dist_clip_voc.py - INFO: \n",
            "configs: {'dataset': {'root_dir': '../VOCdevkit/VOC2012', 'name_list_dir': 'datasets/voc', 'num_classes': 21, 'crop_size': 320, 'resize_range': [512, 2048], 'rescale_range': [0.5, 2.0], 'ignore_index': 255}, 'work_dir': {'dir': 'work_dir_voc', 'ckpt_dir': 'work_dir_voc/checkpoints/2024-11-01-01-23', 'pred_dir': 'work_dir_voc/predictions', 'segs_dir': 'segs', 'tb_logger_dir': 'work_dir_voc/tb_logger/2024-11-01-01-23'}, 'train': {'split': 'train_aug', 'samples_per_gpu': 4, 'max_iters': 30000, 'cam_iters': 2000, 'eval_iters': 2000, 'log_iters': 200}, 'val': {'split': 'train'}, 'optimizer': {'type': 'AdamW', 'learning_rate': 0.0002, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'scheduler': {'warmup_iter': 50, 'warmup_ratio': 1e-06, 'power': 1.0}, 'clip_init': {'clip_pretrain_path': 'ViT-B/16', 'embedding_dim': 256, 'in_channels': [768, 768, 768, 768]}}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "positional_embedding False\n",
            "text_projection False\n",
            "logit_scale False\n",
            "visual.class_embedding False\n",
            "visual.positional_embedding False\n",
            "visual.proj False\n",
            "visual.conv1.weight False\n",
            "visual.ln_pre.weight False\n",
            "visual.ln_pre.bias False\n",
            "visual.transformer.resblocks.0.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.0.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.0.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.0.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.0.ln_1.weight False\n",
            "visual.transformer.resblocks.0.ln_1.bias False\n",
            "visual.transformer.resblocks.0.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.0.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.0.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.0.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.0.ln_2.weight False\n",
            "visual.transformer.resblocks.0.ln_2.bias False\n",
            "visual.transformer.resblocks.1.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.1.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.1.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.1.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.1.ln_1.weight False\n",
            "visual.transformer.resblocks.1.ln_1.bias False\n",
            "visual.transformer.resblocks.1.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.1.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.1.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.1.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.1.ln_2.weight False\n",
            "visual.transformer.resblocks.1.ln_2.bias False\n",
            "visual.transformer.resblocks.2.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.2.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.2.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.2.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.2.ln_1.weight False\n",
            "visual.transformer.resblocks.2.ln_1.bias False\n",
            "visual.transformer.resblocks.2.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.2.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.2.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.2.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.2.ln_2.weight False\n",
            "visual.transformer.resblocks.2.ln_2.bias False\n",
            "visual.transformer.resblocks.3.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.3.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.3.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.3.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.3.ln_1.weight False\n",
            "visual.transformer.resblocks.3.ln_1.bias False\n",
            "visual.transformer.resblocks.3.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.3.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.3.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.3.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.3.ln_2.weight False\n",
            "visual.transformer.resblocks.3.ln_2.bias False\n",
            "visual.transformer.resblocks.4.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.4.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.4.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.4.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.4.ln_1.weight False\n",
            "visual.transformer.resblocks.4.ln_1.bias False\n",
            "visual.transformer.resblocks.4.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.4.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.4.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.4.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.4.ln_2.weight False\n",
            "visual.transformer.resblocks.4.ln_2.bias False\n",
            "visual.transformer.resblocks.5.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.5.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.5.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.5.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.5.ln_1.weight False\n",
            "visual.transformer.resblocks.5.ln_1.bias False\n",
            "visual.transformer.resblocks.5.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.5.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.5.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.5.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.5.ln_2.weight False\n",
            "visual.transformer.resblocks.5.ln_2.bias False\n",
            "visual.transformer.resblocks.6.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.6.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.6.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.6.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.6.ln_1.weight False\n",
            "visual.transformer.resblocks.6.ln_1.bias False\n",
            "visual.transformer.resblocks.6.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.6.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.6.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.6.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.6.ln_2.weight False\n",
            "visual.transformer.resblocks.6.ln_2.bias False\n",
            "visual.transformer.resblocks.7.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.7.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.7.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.7.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.7.ln_1.weight False\n",
            "visual.transformer.resblocks.7.ln_1.bias False\n",
            "visual.transformer.resblocks.7.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.7.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.7.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.7.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.7.ln_2.weight False\n",
            "visual.transformer.resblocks.7.ln_2.bias False\n",
            "visual.transformer.resblocks.8.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.8.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.8.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.8.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.8.ln_1.weight False\n",
            "visual.transformer.resblocks.8.ln_1.bias False\n",
            "visual.transformer.resblocks.8.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.8.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.8.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.8.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.8.ln_2.weight False\n",
            "visual.transformer.resblocks.8.ln_2.bias False\n",
            "visual.transformer.resblocks.9.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.9.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.9.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.9.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.9.ln_1.weight False\n",
            "visual.transformer.resblocks.9.ln_1.bias False\n",
            "visual.transformer.resblocks.9.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.9.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.9.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.9.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.9.ln_2.weight False\n",
            "visual.transformer.resblocks.9.ln_2.bias False\n",
            "visual.transformer.resblocks.10.attn.in_proj_weight False\n",
            "visual.transformer.resblocks.10.attn.in_proj_bias False\n",
            "visual.transformer.resblocks.10.attn.out_proj.weight False\n",
            "visual.transformer.resblocks.10.attn.out_proj.bias False\n",
            "visual.transformer.resblocks.10.ln_1.weight False\n",
            "visual.transformer.resblocks.10.ln_1.bias False\n",
            "visual.transformer.resblocks.10.mlp.c_fc.weight False\n",
            "visual.transformer.resblocks.10.mlp.c_fc.bias False\n",
            "visual.transformer.resblocks.10.mlp.c_proj.weight False\n",
            "visual.transformer.resblocks.10.mlp.c_proj.bias False\n",
            "visual.transformer.resblocks.10.ln_2.weight False\n",
            "visual.transformer.resblocks.10.ln_2.bias False\n",
            "visual.transformer.resblocks.11.attn.in_proj_weight True\n",
            "visual.transformer.resblocks.11.attn.in_proj_bias True\n",
            "visual.transformer.resblocks.11.attn.out_proj.weight True\n",
            "visual.transformer.resblocks.11.attn.out_proj.bias True\n",
            "visual.transformer.resblocks.11.ln_1.weight True\n",
            "visual.transformer.resblocks.11.ln_1.bias True\n",
            "visual.transformer.resblocks.11.mlp.c_fc.weight True\n",
            "visual.transformer.resblocks.11.mlp.c_fc.bias True\n",
            "visual.transformer.resblocks.11.mlp.c_proj.weight True\n",
            "visual.transformer.resblocks.11.mlp.c_proj.bias True\n",
            "visual.transformer.resblocks.11.ln_2.weight True\n",
            "visual.transformer.resblocks.11.ln_2.bias True\n",
            "visual.ln_post.weight False\n",
            "visual.ln_post.bias False\n",
            "transformer.resblocks.0.attn.in_proj_weight False\n",
            "transformer.resblocks.0.attn.in_proj_bias False\n",
            "transformer.resblocks.0.attn.out_proj.weight False\n",
            "transformer.resblocks.0.attn.out_proj.bias False\n",
            "transformer.resblocks.0.ln_1.weight False\n",
            "transformer.resblocks.0.ln_1.bias False\n",
            "transformer.resblocks.0.mlp.c_fc.weight False\n",
            "transformer.resblocks.0.mlp.c_fc.bias False\n",
            "transformer.resblocks.0.mlp.c_proj.weight False\n",
            "transformer.resblocks.0.mlp.c_proj.bias False\n",
            "transformer.resblocks.0.ln_2.weight False\n",
            "transformer.resblocks.0.ln_2.bias False\n",
            "transformer.resblocks.1.attn.in_proj_weight False\n",
            "transformer.resblocks.1.attn.in_proj_bias False\n",
            "transformer.resblocks.1.attn.out_proj.weight False\n",
            "transformer.resblocks.1.attn.out_proj.bias False\n",
            "transformer.resblocks.1.ln_1.weight False\n",
            "transformer.resblocks.1.ln_1.bias False\n",
            "transformer.resblocks.1.mlp.c_fc.weight False\n",
            "transformer.resblocks.1.mlp.c_fc.bias False\n",
            "transformer.resblocks.1.mlp.c_proj.weight False\n",
            "transformer.resblocks.1.mlp.c_proj.bias False\n",
            "transformer.resblocks.1.ln_2.weight False\n",
            "transformer.resblocks.1.ln_2.bias False\n",
            "transformer.resblocks.2.attn.in_proj_weight False\n",
            "transformer.resblocks.2.attn.in_proj_bias False\n",
            "transformer.resblocks.2.attn.out_proj.weight False\n",
            "transformer.resblocks.2.attn.out_proj.bias False\n",
            "transformer.resblocks.2.ln_1.weight False\n",
            "transformer.resblocks.2.ln_1.bias False\n",
            "transformer.resblocks.2.mlp.c_fc.weight False\n",
            "transformer.resblocks.2.mlp.c_fc.bias False\n",
            "transformer.resblocks.2.mlp.c_proj.weight False\n",
            "transformer.resblocks.2.mlp.c_proj.bias False\n",
            "transformer.resblocks.2.ln_2.weight False\n",
            "transformer.resblocks.2.ln_2.bias False\n",
            "transformer.resblocks.3.attn.in_proj_weight False\n",
            "transformer.resblocks.3.attn.in_proj_bias False\n",
            "transformer.resblocks.3.attn.out_proj.weight False\n",
            "transformer.resblocks.3.attn.out_proj.bias False\n",
            "transformer.resblocks.3.ln_1.weight False\n",
            "transformer.resblocks.3.ln_1.bias False\n",
            "transformer.resblocks.3.mlp.c_fc.weight False\n",
            "transformer.resblocks.3.mlp.c_fc.bias False\n",
            "transformer.resblocks.3.mlp.c_proj.weight False\n",
            "transformer.resblocks.3.mlp.c_proj.bias False\n",
            "transformer.resblocks.3.ln_2.weight False\n",
            "transformer.resblocks.3.ln_2.bias False\n",
            "transformer.resblocks.4.attn.in_proj_weight False\n",
            "transformer.resblocks.4.attn.in_proj_bias False\n",
            "transformer.resblocks.4.attn.out_proj.weight False\n",
            "transformer.resblocks.4.attn.out_proj.bias False\n",
            "transformer.resblocks.4.ln_1.weight False\n",
            "transformer.resblocks.4.ln_1.bias False\n",
            "transformer.resblocks.4.mlp.c_fc.weight False\n",
            "transformer.resblocks.4.mlp.c_fc.bias False\n",
            "transformer.resblocks.4.mlp.c_proj.weight False\n",
            "transformer.resblocks.4.mlp.c_proj.bias False\n",
            "transformer.resblocks.4.ln_2.weight False\n",
            "transformer.resblocks.4.ln_2.bias False\n",
            "transformer.resblocks.5.attn.in_proj_weight False\n",
            "transformer.resblocks.5.attn.in_proj_bias False\n",
            "transformer.resblocks.5.attn.out_proj.weight False\n",
            "transformer.resblocks.5.attn.out_proj.bias False\n",
            "transformer.resblocks.5.ln_1.weight False\n",
            "transformer.resblocks.5.ln_1.bias False\n",
            "transformer.resblocks.5.mlp.c_fc.weight False\n",
            "transformer.resblocks.5.mlp.c_fc.bias False\n",
            "transformer.resblocks.5.mlp.c_proj.weight False\n",
            "transformer.resblocks.5.mlp.c_proj.bias False\n",
            "transformer.resblocks.5.ln_2.weight False\n",
            "transformer.resblocks.5.ln_2.bias False\n",
            "transformer.resblocks.6.attn.in_proj_weight False\n",
            "transformer.resblocks.6.attn.in_proj_bias False\n",
            "transformer.resblocks.6.attn.out_proj.weight False\n",
            "transformer.resblocks.6.attn.out_proj.bias False\n",
            "transformer.resblocks.6.ln_1.weight False\n",
            "transformer.resblocks.6.ln_1.bias False\n",
            "transformer.resblocks.6.mlp.c_fc.weight False\n",
            "transformer.resblocks.6.mlp.c_fc.bias False\n",
            "transformer.resblocks.6.mlp.c_proj.weight False\n",
            "transformer.resblocks.6.mlp.c_proj.bias False\n",
            "transformer.resblocks.6.ln_2.weight False\n",
            "transformer.resblocks.6.ln_2.bias False\n",
            "transformer.resblocks.7.attn.in_proj_weight False\n",
            "transformer.resblocks.7.attn.in_proj_bias False\n",
            "transformer.resblocks.7.attn.out_proj.weight False\n",
            "transformer.resblocks.7.attn.out_proj.bias False\n",
            "transformer.resblocks.7.ln_1.weight False\n",
            "transformer.resblocks.7.ln_1.bias False\n",
            "transformer.resblocks.7.mlp.c_fc.weight False\n",
            "transformer.resblocks.7.mlp.c_fc.bias False\n",
            "transformer.resblocks.7.mlp.c_proj.weight False\n",
            "transformer.resblocks.7.mlp.c_proj.bias False\n",
            "transformer.resblocks.7.ln_2.weight False\n",
            "transformer.resblocks.7.ln_2.bias False\n",
            "transformer.resblocks.8.attn.in_proj_weight False\n",
            "transformer.resblocks.8.attn.in_proj_bias False\n",
            "transformer.resblocks.8.attn.out_proj.weight False\n",
            "transformer.resblocks.8.attn.out_proj.bias False\n",
            "transformer.resblocks.8.ln_1.weight False\n",
            "transformer.resblocks.8.ln_1.bias False\n",
            "transformer.resblocks.8.mlp.c_fc.weight False\n",
            "transformer.resblocks.8.mlp.c_fc.bias False\n",
            "transformer.resblocks.8.mlp.c_proj.weight False\n",
            "transformer.resblocks.8.mlp.c_proj.bias False\n",
            "transformer.resblocks.8.ln_2.weight False\n",
            "transformer.resblocks.8.ln_2.bias False\n",
            "transformer.resblocks.9.attn.in_proj_weight False\n",
            "transformer.resblocks.9.attn.in_proj_bias False\n",
            "transformer.resblocks.9.attn.out_proj.weight False\n",
            "transformer.resblocks.9.attn.out_proj.bias False\n",
            "transformer.resblocks.9.ln_1.weight False\n",
            "transformer.resblocks.9.ln_1.bias False\n",
            "transformer.resblocks.9.mlp.c_fc.weight False\n",
            "transformer.resblocks.9.mlp.c_fc.bias False\n",
            "transformer.resblocks.9.mlp.c_proj.weight False\n",
            "transformer.resblocks.9.mlp.c_proj.bias False\n",
            "transformer.resblocks.9.ln_2.weight False\n",
            "transformer.resblocks.9.ln_2.bias False\n",
            "transformer.resblocks.10.attn.in_proj_weight False\n",
            "transformer.resblocks.10.attn.in_proj_bias False\n",
            "transformer.resblocks.10.attn.out_proj.weight False\n",
            "transformer.resblocks.10.attn.out_proj.bias False\n",
            "transformer.resblocks.10.ln_1.weight False\n",
            "transformer.resblocks.10.ln_1.bias False\n",
            "transformer.resblocks.10.mlp.c_fc.weight False\n",
            "transformer.resblocks.10.mlp.c_fc.bias False\n",
            "transformer.resblocks.10.mlp.c_proj.weight False\n",
            "transformer.resblocks.10.mlp.c_proj.bias False\n",
            "transformer.resblocks.10.ln_2.weight False\n",
            "transformer.resblocks.10.ln_2.bias False\n",
            "transformer.resblocks.11.attn.in_proj_weight True\n",
            "transformer.resblocks.11.attn.in_proj_bias True\n",
            "transformer.resblocks.11.attn.out_proj.weight True\n",
            "transformer.resblocks.11.attn.out_proj.bias True\n",
            "transformer.resblocks.11.ln_1.weight True\n",
            "transformer.resblocks.11.ln_1.bias True\n",
            "transformer.resblocks.11.mlp.c_fc.weight True\n",
            "transformer.resblocks.11.mlp.c_fc.bias True\n",
            "transformer.resblocks.11.mlp.c_proj.weight True\n",
            "transformer.resblocks.11.mlp.c_proj.bias True\n",
            "transformer.resblocks.11.ln_2.weight True\n",
            "transformer.resblocks.11.ln_2.bias True\n",
            "token_embedding.weight False\n",
            "ln_final.weight False\n",
            "ln_final.bias False\n",
            "2024-11-01 01:23:24,152 - dist_clip_voc.py - INFO: \n",
            "Network config: \n",
            "WeCLIP(\n",
            "  (encoder): CLIP(\n",
            "    (visual): VisionTransformer(\n",
            "      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
            "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (transformer): Transformer(\n",
            "        (resblocks): Sequential(\n",
            "          (0): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (6): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (7): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (8): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (9): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (10): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (11): ResidualAttentionBlock(\n",
            "            (attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (gelu): QuickGELU()\n",
            "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (resblocks): Sequential(\n",
            "        (0): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (6): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (7): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (8): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (9): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (10): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (11): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (token_embedding): Embedding(49408, 512)\n",
            "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder_fts_fuse): SegFormerHead(\n",
            "    (linears_modulelist): ModuleList(\n",
            "      (0-10): 11 x MLP(\n",
            "        (proj): Linear(in_features=768, out_features=256, bias=True)\n",
            "        (proj_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (linear_fuse): Conv2d(2816, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): DecoderTransformer(\n",
            "    (transformer): Transformer(\n",
            "      (resblocks): Sequential(\n",
            "        (0): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): ResidualAttentionBlock(\n",
            "          (attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Sequential(\n",
            "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (gelu): QuickGELU()\n",
            "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          )\n",
            "          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (linear_pred): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (par): PAR()\n",
            ")\n",
            "2024-11-01 01:23:24,271 - dist_clip_voc.py - INFO: \n",
            "Optimizer: \n",
            "PolyWarmupAdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.999]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.0002\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.999]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 2\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.999]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.002\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "\n",
            "Parameter Group 3\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.999]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.002\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3769: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "2024-11-01 01:25:44,113 - dist_clip_voc.py - INFO: Iter: 200; Elasped: 0:02:27; ETA: 6:05:03; LR: 1.987e-04;, pseudo_seg_loss: 1.7759, attn_loss: 0.4469, pseudo_seg_mAcc: 0.8741\n",
            "2024-11-01 01:27:57,918 - dist_clip_voc.py - INFO: Iter: 400; Elasped: 0:04:40; ETA: 5:45:20; LR: 1.973e-04;, pseudo_seg_loss: 1.0533, attn_loss: 0.4002, pseudo_seg_mAcc: 0.8876\n",
            "2024-11-01 01:30:11,537 - dist_clip_voc.py - INFO: Iter: 600; Elasped: 0:06:54; ETA: 5:38:06; LR: 1.960e-04;, pseudo_seg_loss: 0.8803, attn_loss: 0.3555, pseudo_seg_mAcc: 0.7310\n",
            "2024-11-01 01:32:25,517 - dist_clip_voc.py - INFO: Iter: 800; Elasped: 0:09:08; ETA: 5:33:22; LR: 1.947e-04;, pseudo_seg_loss: 0.9919, attn_loss: 0.3556, pseudo_seg_mAcc: 0.8901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89wX3hCps7dA"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Fig.2 shows four main modules:\n",
        "- a **frozen CLIP backbone** (image encoder and text encoder) to *encode the image and text*,\n",
        "- a **classification process** to produce *initial CAM*,\n",
        "- a **decoder** to generate *segmentation predictions*,\n",
        "- a **RFM** to *refine initial CAM* to provide pseudo labels for training.\n",
        "\n",
        "\n",
        "The Training pipeline is divided into the following steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First of all, the **image is input** to the CLIP image encoder for image features.\n",
        "    - Besides, the *foreground and background class labels are used to build text prompts* and then input to the CLIP text encoder to *generate the corresponding text features*.\n",
        "    - Note here both image and text encoders are **frozen** during training."
      ],
      "metadata": {
        "id": "LEzTkp8L4Hj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Then, the classification scores are generated by **computing distances between image features** (after pooling) **and text features**.\n",
        "    - Based on classification scores, *Grad-CAM is utilized to generate the initial CAM*."
      ],
      "metadata": {
        "id": "sGEgnPFY396A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Besides, *image features from the last layer of each transformer block* in the frozen CLIP image encoder are **input to our proposed decoder for the final segmentation predictions**."
      ],
      "metadata": {
        "id": "MP1R1Lx538o9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Simultaneously, the *intermediate feature maps* from our decoder are used to **generate an affinity map**.\n",
        "    - Then, the *affinity map is **input to our proposed RFM*** with the *multi-head attention maps* from each block of the frozen CLIP image encoder.\n"
      ],
      "metadata": {
        "id": "7xxQl_Ds34KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Finally, **RFM outputs a refining map to refine the initial CAM**.\n",
        "    - After post-processing, the final converted **pseudo label from refined CAM is used to supervise the training**."
      ],
      "metadata": {
        "id": "s5T5rFHy3zEP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hW6xH_yw3gtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frozen CLIP Feature Decoder\n",
        "Frozen CLIP encoder with ViT-B not optimized during training.\n",
        "\n",
        "Suppose the input image is $I \\in \\mathbb R^{3 \\times H \\times W}$ with $H$ height, $W$ width.\n",
        "\n",
        "After passing the CLIP image encoder, we get the image feature maps $\\{F_{init}^l\\}^N_{l=1}$ with $N$ transformer blocks.\n",
        "\n",
        "Then, for each feature map $F_{init}^l$ an individual MLP module is used to generate new corresponding feature maps $F_{new}^l$:\n",
        "$$F_{new}^l = W_{fc}^1 (ReLU(W_{fc}^2(F_{init}^l)))$$\n",
        "where $W_{fc}^1$ and $W_{fc}^2$ are two different fully-connected layers.\n",
        "\n",
        "After that, all new feature maps $\\{F_{new}^l\\}^N_{l=1}$ are concatenated together, which are then processed by a convolution layer to generate a fused feature map $F_{u}$:\n",
        "$$F_u = Conv(Concat[F_{new}^1, F_{new}^2, ..., F_{new}^N])$$\n",
        "where $F_u \\in \\mathbb R^{d \\times h \\times w}$ with $d$ channels, $h$ height, and $w$ width of the feature map. $Conv(\\cdot)$ is a convolutional layer, and $Concat[\\cdot]$ is the concatenation operation.\n",
        "\n",
        "Finally, we design several sequential multi-head transformer layers to generate the final prediction $P$:\n",
        "$$P = Conv(\\phi(F_u)) \\uparrow$$\n",
        "where $P \\in \\mathbb R^{C \\times H \\times W}$, $C$ is the class number including background,    \n",
        "$\\phi( \\cdot )$ represents the sequential multi-head transformer blocks, each block contains:\n",
        "- a multi-head self-attention module,\n",
        "- a feed-forward network,\n",
        "- two normalization layers\n",
        "\n",
        "and $\\uparrow$ is the upsampling operation to align the prediction map size with the original image.\n"
      ],
      "metadata": {
        "id": "uFop9ZkS2m41"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QNFbhhAs7dB"
      },
      "source": [
        "## Frozen CLIP CAM Refinement\n",
        "To provide supervision for the prediction P in Eq. (3), we generate the pixel-level pseudo label from the initial CAM of the frozen backbone.\n",
        "\n",
        "The frozen backbone can only provide static CAM, which means pseudo labels used as supervision cannot be improved during training. The same errors in pseudo labels lead to uncorrectable optimization in the wrong directions. Therefore, we design the Frozen CLIP CAM Refinement module (RFM) to **dynamically update CAM to improve the quality of pseudo labels**.\n",
        "\n",
        "\n",
        "We first follow [Clip is also an efficient segmenter](https://arxiv.org/abs/2212.09506) to generate the initial CAM.\n",
        "\n",
        "For the given image $I$ with its class labels, $I$ is input to the CLIP image encoder.\\\n",
        "The class labels are used to build text prompts and input to the CLIP text encoder.\n",
        "\n",
        "Then, the extracted image features (after pooling) and text features are used to compute the distance and further activated by the softmax function to get the classification scores.\n",
        "\n",
        "After that, we use GradCAM to generate the initial CAM $M_{init} \\in \\mathbb R^{(|C_I|+1) \\times h \\times w}$ where $(|C_I|+1)$ indicates all class labels in the image $I$ including the background class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub7UlFGCs7dB"
      },
      "source": [
        "To thoroughly utilize the prior knowledge of CLIP, the CLIP model is fixed.\\\n",
        "Although we find that such a frozen backbone can provide strong semantic features for the initial CAM with only image-level labels, as illustrated in Fig. 3(a), $M_{init}$ cannot be optimized as it is generated from the frozen backbone, limiting the quality of pseudo labels.\\\n",
        "Therefore, how to rectify $M_{init}$ during training becomes a key issue.\n",
        "\n",
        "Our intuition is to **use feature relationships to rectify the initial CAM**.\\\n",
        "- However, we cannot directly use the attention maps from the CLIP image encoder as the feature relationship, as such attention maps are also fixed.\n",
        "\n",
        "Nevertheless, the decoder is constantly being optimized, and we attempt to use its features to establish feature relationships to guide the selection of attention values from the CLIP image encoder, keeping useful prior CLIP knowledge and removing noisy relationships.\n",
        "\n",
        "With more reliable feature relationships, the CAM quality can be dynamically enhanced.\n",
        "\n",
        "In detail, we first generate an affinity map based on the feature map $F_u$ in Eq. (2) from our decoder:\n",
        "$$A_f = \\text{Sigmoid}(F_u^T F_u)$$\n",
        "where:\n",
        "- $F_u \\in \\mathbb R^{d \\times h \\times w}$ is first flattened to $\\mathbb R^{d \\times hw}$\n",
        "- Sigmoid(·) is the sigmoid function to guarantee the range of the output is from 0 to 1.\n",
        "- $A_f \\in \\mathbb R^{hw \\times hw}$ is the affinity map.\n",
        "- $T$ means matrix transpose.\n",
        "\n",
        "Then we extract all the multi-head attention mpas from the frozen CLIP image encoder, denoted as $\\{A_s^l\\}_{l=1}^N$ and each $A_s^l \\in \\mathbb R^{d \\times hw \\times ww}$.\\\n",
        "For each $A_s^l$, we use $A_f$ as a standard map to evaluate its quality:\n",
        "$$S^l = \\sum_{i=1}^{hw}\\sum_{j=1}^{hw} |A_f(i,j) - A_s^l(i,j)|$$\n",
        "\n",
        "We use the above $S^l$ to compute a filter for each attention map:\n",
        "$$\n",
        "G^l = \\begin{cases}\n",
        "1 & \\text{if } S^l < \\frac{1}{N - N_0 + 1} \\sum_{l = N_0}^N S^l \\\\\n",
        "0 & \\text{else}\n",
        "\\end{cases}\n",
        "$$\n",
        "where $G^l \\in \\mathbb R^{1 \\times 1}$, and it is expanded to $G_e^l \\in \\mathbb R^{hw \\times hw}$ for further computation.\n",
        "\n",
        "We use the average value of all S^l as the threshold.\n",
        "- If the current $S^l$ is less than the threshold, it is more reliable, and we set its filter value as 1.\n",
        "- Otherwise, we set the filter value as 0.\n",
        "\n",
        "Based on this rule, we keep high-quality attention maps and remove weak attention maps.\n",
        "\n",
        "We then combine $A_f$ and the above operation to build the refining map:\n",
        "$$R = \\frac{A_f}{N_m} \\sum_{l=1}^{N} G_e^l A_s^l$$\n",
        "where $N_m$ is the number of valid $A_s^l$, i.e., $N_m = \\sum_{l=N_0}^{N} G^l$\n",
        "\n",
        "Then, following the previous apporaches, we generate the refined CAM:\n",
        "$$M_f^c = \\left(\\frac{R_{nor} + R_{nor}^T}{2}\\right)^{\\alpha} \\cdot M_{init}^c$$\n",
        "where:\n",
        "- $c$ is the specified class,\n",
        "- $M_f^c$ is the refined CAM for class $c$,\n",
        "- $R_{nor}$ is obtained from $R$ using row and column normalization ([Sinkhorn normalization](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-2/A-Relationship-Between-Arbitrary-Positive-Matrices-and-Doubly-Stochastic-Matrices/10.1214/aoms/1177703591.full))\n",
        "- $\\alpha$ is a hyperparameter.\n",
        "\n",
        "This part passes a box mask indicator to restrict the refining region.\n",
        "\n",
        "- $M_{init}^c$ is the CAM for class $c$ after reshaping to $\\mathbb R^{hw \\times 1}$\n",
        "\n",
        "Finally, $M_f$ is input to the online post-processing module, i.e., pixel adaptive refinement module proposed in [Learning affinity from attention](https://arxiv.org/abs/2203.02664), to generate final online pseudo labels $M_p \\in \\mathbb R^{h \\times w}$.\n",
        "\n",
        "In this way, our RFM uses the updated feature relationship in our decoder to assess the feature relationship in the frozen backbone to select reliable relationships.\\\n",
        "Then, higher-quality CAM can be generated with the help of more reliable feature relationships for each image. Fig. 3 shows the detailed comparison of generated CAM using different refinement methods.\\\n",
        "Our method generates more accurate responses than the static refinement method proposed in [29] and the initial CAM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPDZCYgWs7dC"
      },
      "source": [
        "## Loss Function\n",
        "In our RFM, we use the affinity map $A_f$ to select the attention map and build the final refining map. Therefore, the effectiveness of $A_f$ directly determines the quality of the online pseudo labels. Considering $A_f$ is generated using the feature map $F_u$ in our decoder, and is a learnable module, we propose a learning process for $A_f$ that uses the converted online pseudo label from $M_p$ as supervision.\n",
        "\n",
        "Specifically, $M_p$ is first converted to the pixel-wise affinity label for each pair of pixels:\n",
        "$$\\hat A = O_h(M_p)^T O_h(M_p)$$\n",
        "where\n",
        "- $O_h(\\cdot)$ is one-hot encoding and $O_h(M_p) \\in \\mathbb R^{C \\times hw}$,\n",
        "- $\\hat A \\in \\mathbb R^{hw \\times hw}$ is the affinity label.\n",
        "- $\\hat A(i,j) = 1$ means pixel $i$ and $j$ has the same label, otherwise, $\\hat A(i,j) = 0$\n",
        "\n",
        "Based on the above label $\\hat A$ and the online label $M_p$, the whole loss function of our WeCLIP is:\n",
        "$$\n",
        "\\mathcal L = \\mathcal L_{ce}(P, M_p, \\uparrow) + \\lambda \\mathcal L_{ce}(A_f, \\hat A)\n",
        "$$\n",
        "where:\n",
        "- $\\mathcal L_{ce}$ is the cross-entropy loss,\n",
        "- $M_p \\uparrow \\in \\mathbb R^{H\\times W}$\n",
        "- $\\lambda$ is the weighting parameter.\n",
        "- $P$ is the prediction map generated by the decoder\n",
        "\n",
        "With this loss, more accurate feature relationships are established for higher-quality pseudo labels.\\\n",
        "In turn, with better pseudo labels, more precise feature relationships are established. Thus, our decoder and RFM can benefit from each other to boost the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF2xUaQDs7dD"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW1cv1H6s7dD"
      },
      "source": [
        "## Datasets\n",
        "PASCAL VOC 2012 is appended with SBD [17] to expand the dataset, and the whole dataset contains 10,582 training images, 1,446 validation images, and 1,456 test images with 20 foreground classes.\n",
        "\n",
        "The MS COCO-2014 dataset includes approximately 82,000 training images and 40,504 validation images with 80 foreground classes.\n",
        "\n",
        "*Mean Intersection-over-Union (mIoU)* is applied as the evaluation criterion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ks3tTibs7dD"
      },
      "source": [
        "## Implementation Details\n",
        "We use the frozen CLIP backbone with:\n",
        "- the ViT-16-base architecture [13],\n",
        "- N is a fixed number that equals 12.\n",
        "\n",
        "For training on the PASCAL VOC 2012 dataset:\n",
        "- the batchsize is set as 4,\n",
        "- the maximum iteration is set as 30000.\n",
        "\n",
        "For training on the MS COCO-2014 dataset:\n",
        "- we set batchsize as 8,\n",
        "- the maximum iteration as 80000.\n",
        "\n",
        "We follow CLIP-ES [29] to define the background class set.\n",
        "- For PASCAL VOC 2012 set, the set is *{ground, land, grass, tree, building, wall, sky, lake, water, river, sea, railway, railroad, keyboard, helmet, cloud, house, mountain, ocean, road, rock, street, valley, bridge, sign}*\n",
        "- For MS COCO-2014, *{sign, keyboard}* is removed.\n",
        "- Besides, the text prompt for the background class is *‘a clear origami {background class}’*.\n",
        "\n",
        "All other settings adopt the same parameters for two datasets during training:\n",
        "- We use AdamW [32] as the optimizer,\n",
        "    - the learning rate is 2e−3\n",
        "    - with weight decay 1e−3,\n",
        "- all images are cropped to 320 × 320 during training.\n",
        "- λ in Eq. (10) is set as 0.1,\n",
        "\n",
        "- The dimension of the MLP module (Eq. (1)) in our decoder is set as 256.\n",
        "- In $\\phi$ of Eq. (3), three transformer encoder (the multi-head number is 8) layers are cascaded to generate the final feature map,\n",
        "    - each layer’s output dimension is 256.\n",
        "- $N_0$ in Eq. (6) is set as 6.\n",
        "- $\\alpha$ is set as 2 in Eq. (8) following [29].\n",
        "\n",
        "During inference, we use the multi-scale with $\\{0.75, 1.0\\}$. Following previous approaches [39, 40, 53], *DenseCRF* [21] is used as the post-processing method to refine the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MaKTbdHs7dE"
      },
      "source": [
        "## Initial CAM Generation\n",
        "\n",
        "For a given image $I$ with class label set $C_I$, the image is input to the frozen CLIP image encoder to generate the image feature map as $F \\in \\mathbb R^{d \\times (hw)$, after passing global average pooling, the feature vector $F_v \\in \\mathbb R \\times 1$ is generated.\n",
        "\n",
        "Meanwhile, the class labels set $C_I$, with the pre-defined background label set $C_{bg}$, are used to build text prompts using the text \"a clear origami {*}\", where * is the specific class label.\n",
        "\n",
        "Then the text prompts are iput to the text encoder togenerate the feature map $F_t \\in \\mathbb R^{d \\times (|C_I| + |C_{bg}|)}$.\n",
        "\n",
        "Using $F_v$ and $F_t$, the distance is compute as:\n",
        "$$\n",
        "D = \\frac{F_t F_v^T}{||F_t|| \\cdot ||F_v||}\n",
        "$$\n",
        "\n",
        "Then, the distance is passed to the softmax function to generate the class scores:\n",
        "$$\n",
        "S^c = \\text{Sotfmax}(D / \\tau)\n",
        "$$\n",
        "where $S^c$ is the classifcation score for class $c$, and $c \\in \\{C_{bg}, C_I\\}$, $\\tau$ is the temperature parameter.\n",
        "\n",
        "Using GradCAM, we can generate the feature weight map for a specific class $c$ in the $k$th channel:\n",
        "$$\n",
        "w_c^k = \\frac{1}{hw} \\sum_{i=1}^h \\sum_{j=1}^w \\sum_{c'} \\frac{\\partial S^c}{\\partial D^{c'}} \\frac{\\partial D^{c'}}{\\partial F^{k}_{i,j}}\n",
        "$$\n",
        "where $c \\in \\{C_{bg}, C_I\\}$ and $c' \\in \\{C_{bg}, C_I\\}$\n",
        "\n",
        "Finally, the initial CAM for the specific foreground class $c$ is computed as:\n",
        "$$\n",
        "M_{init}^c(i,j) = \\text{ReLU}\\left(\\sum_{k=1}^d w_c^k F_{i, j}^k\\right)\n",
        "$$"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}