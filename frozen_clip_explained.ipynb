{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen CLIP (my attempt, wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import ViT_B_16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giuse\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giuse\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = vit_b_16(pretrained=True)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_pic_path = r\"C:\\01_Learning\\01_Data_science\\01_University\\01_UniTrento\\01_Classes\\Semester\\3\\Advanced_CV\\Code\\dog_pic.jpg\"\n",
    "dog_pic = PIL.Image.open(dog_pic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = ViT_B_16_Weights.DEFAULT.transforms()\n",
    "dog_data = preprocessing(dog_pic).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the first three cells can be replaced by model._process_input(dog_data) -> [1, 196, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 14, 14])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_proj = model.conv_proj(dog_data) \n",
    "conv_proj.shape # (224 - 16)/16 + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size: 16 \n",
      " height: 14 \n",
      " width: 14\n"
     ]
    }
   ],
   "source": [
    "p = model.patch_size\n",
    "n = dog_data.shape[0]\n",
    "n_h = dog_data.shape[2] // p\n",
    "n_w = dog_data.shape[3] // p\n",
    "print(f\"patch_size: {p} \\n height: {n_h} \\n width: {n_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Permute needed because the self attention layer expects inputs in the format (N, S, E)\n",
    "# S: sequence length (number of patches)\n",
    "# E: embedding dimension\n",
    "flattened_conv_proj = conv_proj.view(1, model.hidden_dim, n_h * n_w).permute(0, 2, 1)\n",
    "flattened_conv_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_class_token = model.class_token.expand(n, -1, -1)\n",
    "flattened_conv_proj_with_cls = torch.cat([batch_class_token, flattened_conv_proj], dim=1)\n",
    "flattened_conv_proj_with_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_enc_0[:,1:].mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature maps: torch.Size([14, 14, 768])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_enc_0 = model.encoder.layers.encoder_layer_0(flattened_conv_proj_with_cls)\n",
    "feature_maps_enc_0 = output_enc_0[0, 1:, :].view(n_h, n_w, model.hidden_dim) # remove the class token and reshape to 14x14\n",
    "print(\"feature maps:\",feature_maps_enc_0.shape)\n",
    "patch = T.ToPILImage()(feature_maps_enc_0[:,:,0])\n",
    "patch.resize((224, 224)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 14, 768])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_enc_1 = model.encoder.layers.encoder_layer_1(flattened_conv_proj_with_cls)\n",
    "feature_maps_enc_1 = output_enc_1[0, 1:, :].view(n_h, n_w, model.hidden_dim) # remove the class token and reshape to 14x14\n",
    "feature_maps_enc_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[stack overflow post](https://stackoverflow.com/questions/75874965/how-do-i-extract-features-from-a-torchvision-visitiontransfomer-vit) says that it's not correct and I should only select the cls token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cls_token = output_enc_0[:,-1,:]\n",
    "feature_cls_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get affinity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 14, 1536])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP = nn.Sequential(\n",
    "    nn.Linear(model.hidden_dim, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, model.hidden_dim)\n",
    ")\n",
    "output_MLP_0 = MLP(output_enc_0[0, 1:, :]).view(n_h, n_w, model.hidden_dim)\n",
    "output_MLO_1 = MLP(output_enc_1[0,1:,:]).view(n_h, n_w, model.hidden_dim)\n",
    "output_MLP_concat = torch.cat([output_MLP_0, output_MLO_1], dim=2)\n",
    "output_MLP_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 14, 14])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_F_u = nn.Conv2d(output_MLP_concat.shape[2], model.hidden_dim, kernel_size=1)\n",
    "F_u = conv_F_u(output_MLP_concat.permute(2, 0, 1))\n",
    "F_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 196])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_F_u = F_u.view(model.hidden_dim, n_h * n_w)\n",
    "A_f = flattened_F_u.T @ flattened_F_u\n",
    "A_f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giuse\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from WeCLIP.clip.clip import load # works pretty much like original CLIP implementation, but returns multi-level features and attention maps\n",
    "from WeCLIP.WeCLIP_model.model_attn_aff_coco import WeCLIP\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get feature maps and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(self, image, H, W, require_all_fts=False):\n",
    "        f_x, f_attn = self.visual(image.type(self.dtype), H, W, require_all_fts=require_all_fts)\n",
    "        # f = self.visual(image.type(self.dtype), H, W, require_all_fts=require_all_fts)\n",
    "        return f_x, f_attn\n",
    "\n",
    "def upsample_pos_emb(emb, new_size):\n",
    "    # upsample the pretrained embedding for higher resolution\n",
    "    # emb size NxD\n",
    "    first = emb[:1, :]\n",
    "    emb = emb[1:, :]\n",
    "    N, D = emb.size(0), emb.size(1)\n",
    "    size = int(np.sqrt(N))\n",
    "    assert size * size == N\n",
    "    #new_size = size * self.upsample\n",
    "    emb = emb.permute(1, 0)\n",
    "    emb = emb.view(1, D, size, size).contiguous()\n",
    "    emb = F.upsample(emb, size=new_size, mode='bilinear',)\n",
    "    emb = emb.view(D, -1).contiguous()\n",
    "    emb = emb.permute(1, 0)\n",
    "    emb = torch.cat([first, emb], 0)\n",
    "    emb = nn.parameter.Parameter(emb.half())\n",
    "    return emb\n",
    "\n",
    "def generate_clip_fts(image, model, require_all_fts=True):\n",
    "    model = model.cuda()\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    h, w = image.shape[-2], image.shape[-1]\n",
    "    image = image.cuda()\n",
    "    \n",
    "    image_features_all, attn_weight_list = model.encode_image(image, h, w, require_all_fts=require_all_fts)\n",
    "        \n",
    "    return image_features_all, attn_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_b_16_clip_pretrained = 'ViT-B/16'\n",
    "encoder, preprocess = load(vit_b_16_clip_pretrained, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_pic_path = r\"C:\\01_Learning\\01_Data_science\\01_University\\01_UniTrento\\01_Classes\\Semester\\3\\Advanced_CV\\Code\\dog_pic.jpg\"\n",
    "dog_pic = preprocess(Image.open(dog_pic_path)).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "b, c, h, w = dog_pic.shape\n",
    "new_size = (h//16,w//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giuse\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3782: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embedding_new = upsample_pos_emb(encoder.visual.positional_embedding, new_size)\n",
    "positional_embedding_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 14, 14])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encoder.visual.conv1(dog_pic.type(encoder.dtype)) # patchify\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "x = x.permute(0, 2, 1)\n",
    "x.shape # flatten the patches and permute to (N, Patches, Embedding_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent to (append cls token):\n",
    "# x = torch.cat([encoder.visual.class_embedding.expand(x.shape[0], 1, -1), x], dim=1).dtype(x.dtype), but don't know if lose info this way\n",
    "x = torch.cat([encoder.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x + positional_embedding_new # in the original implementation encoder.visual.positional_embedding_new\n",
    "x = encoder.visual.ln_pre(x) # layer norm\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.permute(1,0,2) # needed to pass to encoder.visual.transformer\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder.visual.transformer.resblocks.forward\n",
    "```\n",
    "attn_output, attn_weight = self.attention(self.ln_1(x))#(L,N,E)  (N,L,L)\n",
    "        x = x + attn_output\n",
    "        x = x + self.mlp(self.ln_2(x)) # linear 768 -> 3072, QuickGELU(), 3072 -> 768\n",
    "        return x, attn_weight\n",
    "```\n",
    "self.attention() (since VisualTransformer self.attn_mask is None):\n",
    "```\n",
    "def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None \n",
    "        return self.attn(x, x, x, need_weights=True, attn_mask=self.attn_mask)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward encoder.visual.transformer\n",
    "# x_all, attn_weights = encoder.visual(dog_pic.type(encoder.dtype), dog_pic.shape[-2], dog_pic.shape[-1], require_all_fts=False)\n",
    "attn_weights = []\n",
    "x_all = []\n",
    "layers = encoder.visual.transformer.layers if x.shape[0] == 77 else encoder.visual.transformer.layers-1\n",
    "for i in range(layers):\n",
    "    x, attn_weight = encoder.visual.transformer.resblocks[i](x) # \n",
    "    x_all.append(x)\n",
    "    attn_weights.append(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the previous steps are equivalent to:\n",
    "image_features_all, attn_weight_list = generate_clip_fts(dog_pic, encoder, require_all_fts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 197]) torch.Size([197, 1, 768]) 11 11\n"
     ]
    }
   ],
   "source": [
    "# length of list = num of residual attention blocks - 1 (not understood why), but still one for each block\n",
    "# multihead attention + layer norm + MLP + layer norm\n",
    "print(attn_weight_list[0].shape, image_features_all[0].shape, len(image_features_all), len(attn_weight_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get CAM and fusing decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 197, 1, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_all_stack = torch.stack(image_features_all,dim=0)\n",
    "fts_all_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 197, 197])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight_stack = torch.stack(attn_weight_list, dim=0).permute(1, 0, 2, 3)\n",
    "attn_weight_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "require_all_fts = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 1, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if require_all_fts == True:\n",
    "    # take always only the last layer for cam\n",
    "    cam_fts_all = fts_all_stack[-1].unsqueeze(0).permute(2, 1, 0, 3)  # (1, hw, 1, c)\n",
    "else:\n",
    "    cam_fts_all = fts_all_stack.permute(2, 1, 0, 3)\n",
    "cam_fts_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 196, 1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_tokens = fts_all_stack[:, 1:, ...] # remove the class token\n",
    "all_img_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tokens_channel = all_img_tokens.size(-1)\n",
    "img_tokens_channel # get embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 768, 196])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_tokens = all_img_tokens.permute(0, 2, 3, 1)\n",
    "all_img_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 768, 14, 14])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_tokens = all_img_tokens.reshape(-1, b, img_tokens_channel, h // 16, w // 16)\n",
    "all_img_tokens.shape # get back patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fusion step of image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "        self.proj_2 = nn.Linear(embed_dim, embed_dim)\n",
    "        # self.proj_3 = nn.Linear(embed_dim*2, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.proj_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerHead(nn.Module):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=128, embedding_dim=256, num_classes=20, index=11, **kwargs):\n",
    "        super(SegFormerHead, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.indexes = index #6 #11\n",
    "\n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n",
    "\n",
    "        linear_layers = [MLP(input_dim=c1_in_channels, embed_dim=embedding_dim) for i in range(self.indexes)]\n",
    "        self.linears_modulelist = nn.ModuleList(linear_layers)\n",
    "\n",
    "        self.linear_fuse = nn.Conv2d(embedding_dim*self.indexes, embedding_dim, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x_all):\n",
    "        x_list = []\n",
    "        for ind in range(x_all.shape[0]):\n",
    "            x = x_all[ind,:, :, :, :]\n",
    "            n, _, h, w = x.shape\n",
    "            _x = self.linears_modulelist[ind](x.float()).permute(0,2,1).reshape(n, -1, x.shape[2], x.shape[3])\n",
    "            x_list.append(_x)\n",
    "        x_list = torch.cat(x_list, dim=1)\n",
    "        x = self.linear_fuse(x_list)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_tokens_emb_dim = all_img_tokens.shape[2]\n",
    "num_feature_maps = all_img_tokens.shape[0]\n",
    "output_embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-10): 11 x MLP(\n",
       "    (proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (proj_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layers = [MLP(input_dim=all_img_tokens_emb_dim, embed_dim=output_embedding_dim) for i in range(num_feature_maps)]\n",
    "linears_modulelist = nn.ModuleList(linear_layers).to(\"cuda\")\n",
    "linears_modulelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(2816, 256, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the output of the MLPs\n",
    "linear_fuse = nn.Conv2d(output_embedding_dim*num_feature_maps, output_embedding_dim, kernel_size=1).to(\"cuda\")\n",
    "linear_fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 14, 14])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_tokens[0,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performed internally by MLP forward\n",
    "all_img_tokens[0,...].float().flatten(2).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 256])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This operation is performed independently for each of the 196 vectors in the sequence\n",
    "linears_modulelist[0](all_img_tokens[0,...].float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2816, 14, 14])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list = []\n",
    "for ind in range(all_img_tokens.shape[0]):\n",
    "    x = all_img_tokens[ind,:, :, :, :]\n",
    "    n, _, h, w = x.shape\n",
    "    _x = linears_modulelist[ind](x.float()).permute(0,2,1).reshape(n, -1, x.shape[2], x.shape[3])\n",
    "    x_list.append(_x)\n",
    "x_list = torch.cat(x_list, dim=1)\n",
    "x_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 14, 14])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_x = linear_fuse(x_list)\n",
    "fused_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "previous steps equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 14, 14])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalentl to:\n",
    "all_img_tokens_emb_dim = all_img_tokens.shape[2]\n",
    "num_feature_maps = all_img_tokens.shape[0]\n",
    "decorder_fts_fuse = SegFormerHead(in_channels=[all_img_tokens_emb_dim, all_img_tokens_emb_dim, all_img_tokens_emb_dim, all_img_tokens_emb_dim],\n",
    "                                  embedding_dim=output_embedding_dim, # output embedding dimension \n",
    "                                  num_classes=20, # doesn't have any influence on the output here\n",
    "                                  index=num_feature_maps).to(\"cuda\")\n",
    "# fuse the features from the encoder\n",
    "fts = decorder_fts_fuse(all_img_tokens)\n",
    "fts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_fts = fts.clone()\n",
    "_, _, fts_h, fts_w = fts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAM computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
