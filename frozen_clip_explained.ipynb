{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(os.getcwd())) # to avoid import error with other directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WeCLIP.WeCLIP_model.Decoder.TransDecoder import DecoderTransformer\n",
    "from WeCLIP.clip.clip import load # works pretty much like original CLIP implementation, but returns multi-level features and attention maps\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get feature maps and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(self, image, H, W, require_all_fts=False):\n",
    "        f_x, f_attn = self.visual(image.type(self.dtype), H, W, require_all_fts=require_all_fts)\n",
    "        # f = self.visual(image.type(self.dtype), H, W, require_all_fts=require_all_fts)\n",
    "        return f_x, f_attn\n",
    "\n",
    "def upsample_pos_emb(emb, new_size):\n",
    "    # upsample the pretrained embedding for higher resolution\n",
    "    # emb size NxD\n",
    "    first = emb[:1, :]\n",
    "    emb = emb[1:, :]\n",
    "    N, D = emb.size(0), emb.size(1)\n",
    "    size = int(np.sqrt(N))\n",
    "    assert size * size == N\n",
    "    #new_size = size * self.upsample\n",
    "    emb = emb.permute(1, 0)\n",
    "    emb = emb.view(1, D, size, size).contiguous()\n",
    "    emb = F.upsample(emb, size=new_size, mode='bilinear',)\n",
    "    emb = emb.view(D, -1).contiguous()\n",
    "    emb = emb.permute(1, 0)\n",
    "    emb = torch.cat([first, emb], 0)\n",
    "    emb = nn.parameter.Parameter(emb.half())\n",
    "    return emb\n",
    "\n",
    "def generate_clip_fts(image, model, require_all_fts=True):\n",
    "    model = model.cuda()\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    h, w = image.shape[-2], image.shape[-1]\n",
    "    image = image.cuda()\n",
    "    \n",
    "    image_features_all, attn_weight_list = model.encode_image(image, h, w, require_all_fts=require_all_fts)\n",
    "        \n",
    "    return image_features_all, attn_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_b_16_clip_pretrained = 'ViT-B/16'\n",
    "encoder, preprocess = load(vit_b_16_clip_pretrained, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_pic_path = r\"C:\\01_Learning\\01_Data_science\\01_University\\01_UniTrento\\01_Classes\\Semester\\3\\Advanced_CV\\Code\\dog_pic.jpg\" # path to one of your images\n",
    "dog_pic = preprocess(Image.open(dog_pic_path)).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "b, c, h, w = dog_pic.shape\n",
    "new_size = (h//16,w//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giuse\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3782: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embedding_new = upsample_pos_emb(encoder.visual.positional_embedding, new_size)\n",
    "positional_embedding_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 14, 14])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encoder.visual.conv1(dog_pic.type(encoder.dtype)) # patchify\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "x = x.permute(0, 2, 1)\n",
    "x.shape # flatten the patches and permute to (N, Patches, Embedding_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent to (append cls token):\n",
    "# x = torch.cat([encoder.visual.class_embedding.expand(x.shape[0], 1, -1), x], dim=1).dtype(x.dtype), but don't know if lose info this way\n",
    "x = torch.cat([encoder.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x + positional_embedding_new # in the original implementation encoder.visual.positional_embedding_new\n",
    "x = encoder.visual.ln_pre(x) # layer norm\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.permute(1,0,2) # needed to pass to encoder.visual.transformer\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder.visual.transformer.resblocks.forward\n",
    "```\n",
    "attn_output, attn_weight = self.attention(self.ln_1(x))#(L,N,E)  (N,L,L)\n",
    "        x = x + attn_output\n",
    "        x = x + self.mlp(self.ln_2(x)) # linear 768 -> 3072, QuickGELU(), 3072 -> 768\n",
    "        return x, attn_weight\n",
    "```\n",
    "self.attention() (since VisualTransformer self.attn_mask is None):\n",
    "```\n",
    "def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None \n",
    "        return self.attn(x, x, x, need_weights=True, attn_mask=self.attn_mask)\n",
    "```\n",
    "attn_weights: set of probabilities that indicate the \"importance\" or \"relevance\" of each input token to every other token in the sequence (row-wise sum to 1) \\\n",
    "attn_output: feature map from a transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward encoder.visual.transformer\n",
    "# x_all, attn_weights = encoder.visual(dog_pic.type(encoder.dtype), dog_pic.shape[-2], dog_pic.shape[-1], require_all_fts=False)\n",
    "\n",
    "attn_weights = []\n",
    "x_all = []\n",
    "layers = encoder.visual.transformer.layers if x.shape[0] == 77 else encoder.visual.transformer.layers-1\n",
    "for i in range(layers):\n",
    "    x, attn_weight = encoder.visual.transformer.resblocks[i](x) # \n",
    "    x_all.append(x)\n",
    "    attn_weights.append(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the previous steps are equivalent to:\n",
    "image_features_all, attn_weight_list = generate_clip_fts(dog_pic, encoder, require_all_fts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 197]) torch.Size([197, 1, 768]) 11 11\n"
     ]
    }
   ],
   "source": [
    "# length of list = num of residual attention blocks - 1 (not understood why), but still one for each block\n",
    "# multihead attention + layer norm + MLP + layer norm\n",
    "print(attn_weight_list[0].shape, image_features_all[0].shape, len(image_features_all), len(attn_weight_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 197, 1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_all_stack = torch.stack(image_features_all,dim=0)\n",
    "fts_all_stack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 196, 1, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_tokens = fts_all_stack[:, 1:, ...] # remove the class token\n",
    "all_img_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tokens_channel = all_img_tokens.size(-1)\n",
    "img_tokens_channel # get embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 768, 196])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_tokens = all_img_tokens.permute(0, 2, 3, 1)\n",
    "all_img_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 768, 14, 14])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_tokens = all_img_tokens.reshape(-1, b, img_tokens_channel, h // 16, w // 16)\n",
    "all_img_tokens.shape # get back patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fusion step of image features\n",
    "- number of trainable parameters of ``SegFormerHead``\n",
    "    - ``linear``: 2.89 M\n",
    "    - ``Conv2d``: 0.72 M\n",
    "    - ``total`` : 3.61 M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "        self.proj_2 = nn.Linear(embed_dim, embed_dim)\n",
    "        # self.proj_3 = nn.Linear(embed_dim*2, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.proj_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerHead(nn.Module):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=128, embedding_dim=256, num_classes=20, index=11, **kwargs):\n",
    "        super(SegFormerHead, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.indexes = index #6 #11\n",
    "\n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n",
    "\n",
    "        linear_layers = [MLP(input_dim=c1_in_channels, embed_dim=embedding_dim) for i in range(self.indexes)]\n",
    "        self.linears_modulelist = nn.ModuleList(linear_layers)\n",
    "\n",
    "        self.linear_fuse = nn.Conv2d(embedding_dim*self.indexes, embedding_dim, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x_all):\n",
    "        x_list = []\n",
    "        for ind in range(x_all.shape[0]):\n",
    "            x = x_all[ind,:, :, :, :]\n",
    "            n, _, h, w = x.shape\n",
    "            _x = self.linears_modulelist[ind](x.float()).permute(0,2,1).reshape(n, -1, x.shape[2], x.shape[3])\n",
    "            x_list.append(_x)\n",
    "        x_list = torch.cat(x_list, dim=1)\n",
    "        x = self.linear_fuse(x_list)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_tokens_emb_dim = all_img_tokens.shape[2]\n",
    "num_feature_maps = all_img_tokens.shape[0]\n",
    "output_embedding_dim = 256 # will also be used to define the width of DecoderTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-10): 11 x MLP(\n",
       "    (proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (proj_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layers = [MLP(input_dim=all_img_tokens_emb_dim, embed_dim=output_embedding_dim) for i in range(num_feature_maps)]\n",
    "linears_modulelist = nn.ModuleList(linear_layers).to(\"cuda\")\n",
    "linears_modulelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(2816, 256, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the output of the MLPs\n",
    "linear_fuse = nn.Conv2d(output_embedding_dim*num_feature_maps, output_embedding_dim, kernel_size=1).to(\"cuda\")\n",
    "linear_fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 14, 14])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_tokens[0,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performed internally by MLP forward\n",
    "all_img_tokens[0,...].float().flatten(2).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 256])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This operation is performed independently for each of the 196 vectors in the sequence\n",
    "linears_modulelist[0](all_img_tokens[0,...].float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2816, 14, 14])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list = []\n",
    "for ind in range(all_img_tokens.shape[0]):\n",
    "    x = all_img_tokens[ind,:, :, :, :]\n",
    "    n, _, h, w = x.shape\n",
    "    _x = linears_modulelist[ind](x.float()).permute(0,2,1).reshape(n, -1, x.shape[2], x.shape[3])\n",
    "    x_list.append(_x)\n",
    "x_list = torch.cat(x_list, dim=1)\n",
    "x_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 14, 14])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_x = linear_fuse(x_list)\n",
    "fused_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "previous steps equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 14, 14])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalentl to:\n",
    "all_img_tokens_emb_dim = all_img_tokens.shape[2]\n",
    "num_feature_maps = all_img_tokens.shape[0]\n",
    "num_classes = 20 \n",
    "decorder_fts_fuse = SegFormerHead(in_channels=[all_img_tokens_emb_dim, all_img_tokens_emb_dim, all_img_tokens_emb_dim, all_img_tokens_emb_dim],\n",
    "                                  embedding_dim=output_embedding_dim, # output embedding dimension \n",
    "                                  num_classes=num_classes, # doesn't have any influence on the output here\n",
    "                                  index=num_feature_maps).to(\"cuda\")\n",
    "# fuse the features from the encoder\n",
    "fts = decorder_fts_fuse(all_img_tokens)\n",
    "fts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decode segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes compressed feature maps, flattens spatial dimensions ``(b, output_embedding_dim, h*w)`` , feeds the data to 3 transformer blocks, reshapes the output ``(b, output_embedding_dim, h, w)`` and pass it to a ``Conv2d -> [b, num_classes, h, w]`` \n",
    "- heads: Number of parallel attention heads. each head will have dimension embed_dim // num_heads -> \\\n",
    "``256 // 8 = 32 == decoder_transform.transformer.resblocks[0].attn.head_dim``\n",
    "- ``decoder_transform.transformer``: transformer layers just like the ones in CLIP\n",
    "- ``decoder_transform.linear_pred``: convolution maps ``embedding -> num_classes``\n",
    "- total number of trainable parameters: 2.37 M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_transform = DecoderTransformer(width = output_embedding_dim, # 256\n",
    "                                       layers = 3, # as suggested in the paper, >3 overfits\n",
    "                                       heads = 8, \n",
    "                                       output_dim = num_classes # same fo SegFormerHead\n",
    "                                       ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_attn_weight_list not used in the repo \n",
    "seg, seg_attn_weight_list = decoder_transform(fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 14, 14])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the output of the decoder that will be compared to the pseudo label after using bilinear interpolation (upsampling) to align the spatial dimensions\n",
    "# the upsampling is performed in the training loop e.g. see WeCLIP/scripts/dist_clip_coco.py line 251\n",
    "seg.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize seg pathces (not in the repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 14, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_plots = (seg[0].cpu().detach().numpy() * 255).astype('uint8')\n",
    "seg_plots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "rows, cols = 5, 5\n",
    "img_height, img_width = seg_plots.shape[1], seg_plots.shape[2]\n",
    "\n",
    "# Create a blank canvas for the grid\n",
    "grid_image = Image.new('L', (cols * img_width, rows * img_height))  # 'L' mode for grayscale\n",
    "\n",
    "# Place each 14x14 image on the grid\n",
    "for idx, img_array in enumerate(seg_plots):\n",
    "    if idx >= rows * cols:  # Stop if we've filled the grid\n",
    "        break\n",
    "    img = Image.fromarray((img_array * 255).astype('uint8'))  # Scale to 0-255\n",
    "    row, col = divmod(idx, cols)\n",
    "    grid_image.paste(img, (col * img_width, row * img_height))\n",
    "\n",
    "# Display the grid\n",
    "# grid_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get affinity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 14, 14])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_fts = fts.clone()\n",
    "attn_fts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_b, f_c, f_h, f_w = attn_fts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 196])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_fts_flatten = attn_fts.reshape(f_b, f_c, f_h*f_w) \n",
    "attn_fts_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 196])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch matrix-matrix product of attn_fts_flatten and its transpose\n",
    "attn_pred = attn_fts_flatten.transpose(2, 1).bmm(attn_fts_flatten) \n",
    "attn_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be used with attn_weight_list in RFM\n",
    "attn_pred = torch.sigmoid(attn_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAM computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 197, 197])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight_stack = torch.stack(attn_weight_list, dim=0).permute(1, 0, 2, 3)\n",
    "attn_weight_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 1, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require_all_fts = True\n",
    "\n",
    "if require_all_fts == True:\n",
    "    # take always only the last layer for cam\n",
    "    cam_fts_all = fts_all_stack[-1].unsqueeze(0).permute(2, 1, 0, 3)  # (1, hw, 1, c)\n",
    "else:\n",
    "    cam_fts_all = fts_all_stack.permute(2, 1, 0, 3)\n",
    "cam_fts_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 1, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_fts_all[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 197, 197])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight_stack[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 196])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_attn = attn_pred.unsqueeze(0)[:, 0, :, :]  # seg_attn for the first image\n",
    "seg_attn.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
