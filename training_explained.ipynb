{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# train on voc\n",
    "python scripts/dist_clip_voc.py --config your/path/WeCLIP/configs/voc_attn_reg.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three parameters requires to be modified based on your path in [voc_attn_reg.yaml](WeCLIP/configs/voc_attn_reg.yaml), \n",
    "```yaml\n",
    "dataset:\n",
    "  root_dir: /your/path/VOCdevkit/VOC2012\n",
    "  name_list_dir: /your/path/WeCLIP/datasets/voc\n",
    "  ...\n",
    "clip_init:\n",
    "  clip_pretrain_path: /your/path/WeCLIP/pretrained/ViT-B-16.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [dist_clip_voc.py](WeCLIP/scripts/dist_clip_voc.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "\n",
    "# main steps, logging omitted\n",
    "def train(cfg):\n",
    "\n",
    "    train_dataset = voc.VOC12ClsDataset(...)\n",
    "    \n",
    "    val_dataset = voc.VOC12SegDataset(...)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, ...)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, ...)\n",
    "\n",
    "    WeCLIP_model = WeCLIP(cfg)\n",
    "\n",
    "    param_groups = WeCLIP_model.get_param_groups()\n",
    "    WeCLIP_model.cuda()\n",
    "\n",
    "    mask_size = int(cfg.dataset.crop_size // 16)\n",
    "    attn_mask = get_mask_by_radius(h=mask_size, w=mask_size, radius=args.radius)\n",
    "\n",
    "    optimizer = PolyWarmupAdamW(\n",
    "        params=[\n",
    "            {\n",
    "                \"params\": param_groups[0],\n",
    "                \"lr\": cfg.optimizer.learning_rate,\n",
    "                \"weight_decay\": cfg.optimizer.weight_decay,\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        lr = cfg.optimizer.learning_rate,\n",
    "        weight_decay = cfg.optimizer.weight_decay,\n",
    "        ...\n",
    "    )\n",
    "\n",
    "    train_loader_iter = iter(train_loader)\n",
    "\n",
    "\n",
    "    for n_iter in range(cfg.train.max_iters):\n",
    "        \n",
    "        img_name, inputs, cls_labels, img_box = next(train_loader_iter)\n",
    "\n",
    "        segs, cam, attn_pred = WeCLIP_model(inputs.cuda(), img_name)\n",
    "\n",
    "        pseudo_label = cam\n",
    "\n",
    "        segs = F.interpolate(segs, size=pseudo_label.shape[1:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        fts_cam = cam.clone()\n",
    "\n",
    "            \n",
    "        aff_label = cams_to_affinity_label(fts_cam, mask=attn_mask, ignore_index=cfg.dataset.ignore_index)\n",
    "        \n",
    "        attn_loss, pos_count, neg_count = get_aff_loss(attn_pred, aff_label)\n",
    "\n",
    "        seg_loss = get_seg_loss(segs, pseudo_label.type(torch.long), ignore_index=cfg.dataset.ignore_index)\n",
    "\n",
    "        loss = 1 * seg_loss + 0.1*attn_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeCLIP_model = [WeCLIP(cfg)](WeCLIP/WeCLIP_model/model_attn_aff_voc.py#L60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class WeCLIP(nn.Module):\n",
    "    def __init__(self, num_classes, clip_model, embedding_dim=256, in_channels=512, dataset_root_path, device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        ## CLIP Encoder\n",
    "        self.encoder, _ = clip.load(clip_model, device=device)\n",
    "        # Freeze all layers except the last layer\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if \"11\" not in name:\n",
    "                param.requires_grad=False\n",
    "\n",
    "        ## Decoded Components\n",
    "        # SegFormerHead: Fuses features from different transformer layers\n",
    "        self.decoder_fts_fuse = SegFormerHead(in_channel, embedding_dim, num_classes, index=11)\n",
    "        \n",
    "        # DecoderTransformer: Processes fused features for segmentation\n",
    "        self.decoder = DecoderTransformer(width=embedding_dim, layers=3, heads=8, output_dim=num_classes)\n",
    "\n",
    "        ## Text Features\n",
    "        # zero-shot classifiers for background and foreground categories\n",
    "        self.bg_text_features = zeroshot_classifier(BACKGROUND_CATEGORY, ['a clean origami {}.'], self.encoder)\n",
    "        self.fg_text_features = zeroshot_classifier(new_class_names, ['a clean origami {}.'], self.encoder)\n",
    "\n",
    "        self.target_layers = [self.encoder.visual.transformer.resblocks[-1].ln_1]\n",
    "\n",
    "        # GradCAM for visualization and init CAM\n",
    "        self.grad_cam = GradCAM(model=self.encoder, target_layers, reshape_transform)\n",
    "        # PAR for refining CAMs\n",
    "        self.par = PAR(num_iter=20, dilations=[1,2,4,8,12,24]).cuda()\n",
    "\n",
    "        self.encoder.eval()\n",
    "        self.cam_bg_thres = 1\n",
    "        self.require_all_fts = True\n",
    "\n",
    "\n",
    "    def forward(self, img, img_names='2007_000032', mode='train'):\n",
    "        \n",
    "        cam_list = []\n",
    "        b, c, h, w = img.shape\n",
    "        self.encoder.eval()                 #NOTE: already in __init__\n",
    "        self.iter_num += 1\n",
    "\n",
    "        # ------------------- Initial Feature Extraction -------------------\n",
    "        fts_all, attn_weight_list = generate_clip_fts(img, self.encoder, require_all_fts=True)\n",
    "        fts_all_stack = torch.stack(fts_all, dim=0) # (11, hw, b, c) because of 11 transformer layers\n",
    "        attn_weight_stack = torch.stack(attn_weight_list, dim=0).permute(1, 0, 2, 3)\n",
    "        \n",
    "        # ------------------- Feature Processing -------------------\n",
    "        if self.require_all_fts==True:      #BUG: Why this if condition?\n",
    "            cam_fts_all = fts_all_stack[-1].unsqueeze(0).permute(2, 1, 0, 3) #(1, hw, 1, c)\n",
    "        else:\n",
    "            cam_fts_all = fts_all_stack.permute(2, 1, 0, 3) #(b, hw, 11, c)\n",
    "\n",
    "        all_img_tokens = fts_all_stack[:, 1:, ...]      # [CLS] not needed for segmentation\n",
    "        img_tokens_channel = all_img_tokens.size(-1)\n",
    "        all_img_tokens = all_img_tokens.permute(0, 2, 3, 1) #BUG : why permute?\n",
    "        all_img_tokens = all_img_tokens.reshape(-1, b, img_tokens_channel, h//16, w //16) #(11, b, c, h, w), 16 is the downsample factor from CLIP\n",
    "        \n",
    "        # ------------------- Decoder Pipeline -------------------\n",
    "        # SegFormerHead for feature fusion\n",
    "        fts = self.decoder_fts_fuse(all_img_tokens)\n",
    "        attn_fts = fts.clone()\n",
    "        _, _, fts_h, fts_w = fts.shape\n",
    "        \n",
    "        # DecoderTransformer for generating segmentation predictions\n",
    "        seg, seg_attn_weight_list = self.decoder(fts)\n",
    "\n",
    "        # ------------------- Attention Computation -------------------\n",
    "        # Computes self-attention scores between all spatial positions\n",
    "        f_b, f_c, f_h, f_w = attn_fts.shape\n",
    "        attn_fts_flatten = attn_fts.reshape(f_b, f_c, f_h*f_w)  # (b, c, hw)\n",
    "        attn_pred = attn_fts_flatten.transpose(2, 1).bmm(attn_fts_flatten)  # (b, hw, hw)\n",
    "        attn_pred = torch.sigmoid(attn_pred)    # interpretable as attention scores\n",
    "\n",
    "        for i, img_name in enumerate(img_names):\n",
    "            img_path = os.path.join(self.root_path, str(img_name)+'.png')\n",
    "            img_i = img[i]  # (3, h, w)\n",
    "            \n",
    "            # ------- 1. Extract CAM features -------\n",
    "            cam_fts = cam_fts_all[i]                        # (hw, 1, c)\n",
    "            cam_attn = attn_weight_stack[i]                 # (11, hw, h, w)\n",
    "            seg_attn = attn_pred.unsqueeze(0)[:, i, :, :]   # (1, hw, hw)\n",
    "            \n",
    "            # ------- 2. Applies refinement after 15000 iterations -------\n",
    "            if self.iter_num > 15000 or mode=='val': #15000\n",
    "                require_seg_trans = True\n",
    "            else:\n",
    "                require_seg_trans = False\n",
    "\n",
    "            cam_refined_list, keys, w, h = perform_single_voc_cam(img_path, img_i, cam_fts, cam_attn, seg_attn,\n",
    "                                                                   self.bg_text_features, self.fg_text_features,\n",
    "                                                                   self.grad_cam,\n",
    "                                                                   mode=mode,\n",
    "                                                                   require_seg_trans=require_seg_trans)\n",
    "\n",
    "            cam_dict = generate_cam_label(cam_refined_list, keys, w, h)\n",
    "            \n",
    "            cams = cam_dict['refined_cam'].cuda()\n",
    "\n",
    "            # ------- 3. Process Background scoring -------\n",
    "            bg_score = torch.pow(1 - torch.max(cams, dim=0, keepdims=True)[0], self.cam_bg_thres).cuda()\n",
    "            cams = torch.cat([bg_score, cams], dim=0).cuda()    # (num_classes+1, h, w)\n",
    "            \n",
    "            # Shifts existing class indices up by 1 to make room for background (0)\n",
    "            # Adds padding at start for background class\n",
    "            valid_key = np.pad(cam_dict['keys'] + 1, (1, 0), mode='constant')\n",
    "            valid_key = torch.from_numpy(valid_key).cuda()\n",
    "            \n",
    "            # ------- 4. Refine CAMs using PAR -------\n",
    "            with torch.no_grad():\n",
    "                cam_labels = _refine_cams(self.par, img[i], cams, valid_key)\n",
    "            \n",
    "            cam_list.append(cam_labels)\n",
    "\n",
    "        all_cam_labels = torch.stack(cam_list, dim=0)\n",
    "\n",
    "        return seg, all_cam_labels, attn_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fts_all, attn_weight_list = [generate_clip_fts](WeCLIP/clip/clip_tool.py#31)(img, self.encoder, require_all_fts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def generate_clip_fts(image, model, require_all_fts=True):\n",
    "    model = model.cuda()\n",
    "\n",
    "    if len(image.shape) == 3:       # (c, h, w)\n",
    "        image = image.unsqueeze(0)  # (1, c, h, w)\n",
    "    h, w = image.shape[-2], image.shape[-1]\n",
    "    image = image.cuda()\n",
    "    \n",
    "    # model = self.encoder = clip.load(clip_model, device=device)\n",
    "    image_features_all, attn_weight_list = model.encode_image(image, h, w, require_all_fts=require_all_fts)\n",
    "        \n",
    "    return image_features_all, attn_weight_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### image_features_all, attn_weight_list = [model.encode_image](WeCLIP/clip/model.py)(image, h, w, require_all_fts=require_all_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(..., vision_layers, ...):\n",
    "        ...\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(...)\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisionTransformer(...)\n",
    "\n",
    "    def encode_image(self, image, H, W, require_all_fts=False):\n",
    "        f_x, f_attn = self.visual(image.type(self.dtype), H, W, require_all_fts=require_all_fts)\n",
    "        # f = self.visual(image.type(self.dtype), H, W, require_all_fts=require_all_fts)\n",
    "        return f_x, f_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### self.visual = [ModifiedResNet($\\cdot$)](WeCLIP/clip/model.py#L114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    # NOTE: layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        def stem(x):\n",
    "            x = self.relu1(self.bn1(self.conv1(x)))\n",
    "            x = self.relu2(self.bn2(self.conv2(x)))\n",
    "            x = self.relu3(self.bn3(self.conv3(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)#(1,,2048, 7, 7)\n",
    "        x_pooled = self.attnpool(x, H, W)\n",
    "\n",
    "        return x_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        ...\n",
    "    \n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### self.visual = [VisionTransformer($ \\cdot $)](WeCLIP/clip/model.py#L246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor, H, W, require_all_fts=False):\n",
    "\n",
    "        self.positional_embedding_new = upsample_pos_emb(self.positional_embedding, (H//16,W//16))\n",
    "        x = self.conv1(x)                               # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)       # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)                          # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)               \n",
    "                                                        # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding_new.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x, attn_weight = self.transformer(x, require_all_fts=require_all_fts)\n",
    "\n",
    "\n",
    "        '''\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "        '''\n",
    "\n",
    "        return x, attn_weight#cls_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def upsample_pos_emb(emb, new_size):\n",
    "    # upsample the pretrained embedding for higher resolution\n",
    "    # emb size NxD\n",
    "    first = emb[:1, :]\n",
    "    emb = emb[1:, :]\n",
    "    N, D = emb.size(0), emb.size(1)\n",
    "    size = int(np.sqrt(N))\n",
    "    assert size * size == N\n",
    "    #new_size = size * self.upsample\n",
    "    emb = emb.permute(1, 0)\n",
    "    emb = emb.view(1, D, size, size).contiguous()\n",
    "    emb = F.upsample(emb, size=new_size, mode='bilinear',)\n",
    "    emb = emb.view(D, -1).contiguous()\n",
    "    emb = emb.permute(1, 0)\n",
    "    emb = torch.cat([first, emb], 0)\n",
    "    emb = nn.parameter.Parameter(emb.half())\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, require_all_fts=False):\n",
    "        attn_weights = []\n",
    "        x_all = []\n",
    "        with torch.no_grad():\n",
    "            layers = self.layers if x.shape[0] == 77 else self.layers-1  # 77 context\n",
    "            for i in range(layers):\n",
    "                x, attn_weight = self.resblocks[i](x)\n",
    "                x_all.append(x)\n",
    "                attn_weights.append(attn_weight)\n",
    "        '''\n",
    "        for i in range(self.layers-1, self.layers):\n",
    "            x, attn_weight = self.resblocks[i](x)\n",
    "            attn_weights.append(attn_weight)\n",
    "            #feature_map_list.append(x)\n",
    "        '''\n",
    "        if require_all_fts == True:\n",
    "            return x_all, attn_weights\n",
    "        else:\n",
    "            return x, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% script True\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.attn = myAtt.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=True, attn_mask=self.attn_mask)#[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        attn_output, attn_weight = self.attention(self.ln_1(x))#(L,N,E)  (N,L,L)\n",
    "        x = x + attn_output\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x, attn_weight\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "class MultiheadAttention(Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "\n",
    "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fts = self.[decoder_fts_fuse(all_img_tokens)](WeCLIP/WeCLIP_model/segformer_head.py#L49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class SegFormerHead(nn.Module):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=128, embedding_dim=256, num_classes=20, index=11, **kwargs):\n",
    "        super(SegFormerHead, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.indexes = index #6 #11\n",
    "\n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n",
    "\n",
    "        linear_layers = [MLP(input_dim=c1_in_channels, embed_dim=embedding_dim) for i in range(self.indexes)]\n",
    "        self.linears_modulelist = nn.ModuleList(linear_layers)\n",
    "\n",
    "        self.linear_fuse = nn.Conv2d(embedding_dim*self.indexes, embedding_dim, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x_all):\n",
    "        x_list = []\n",
    "        for ind in range(x_all.shape[0]):\n",
    "            x = x_all[ind,:, :, :, :]\n",
    "            n, _, h, w = x.shape\n",
    "            _x = self.linears_modulelist[ind](x.float()).permute(0,2,1).reshape(n, -1, x.shape[2], x.shape[3])\n",
    "            x_list.append(_x)\n",
    "        x_list = torch.cat(x_list, dim=1)\n",
    "        x = self.linear_fuse(x_list)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "        self.proj_2 = nn.Linear(embed_dim, embed_dim)\n",
    "        # self.proj_3 = nn.Linear(embed_dim*2, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.proj_2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Conv_Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(input_dim, embed_dim, kernel_size=1)\n",
    "        self.proj_2 = nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
    "        # self.proj_3 = nn.Linear(embed_dim*2, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.proj_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seg, seg_attn_weight_list = self.[decoder(fts)](WeCLIP/WeCLIP_model/Decoder/TransDecoder.py#L104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "        # self.dropout = nn.Dropout2d(0.1)\n",
    "        self.linear_pred = nn.Conv2d(width, output_dim, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.reshape(b, c, h*w)    # NDL\n",
    "        x = x.permute(2, 0, 1)      # NDL -> LND\n",
    "        \n",
    "        x, attn_weights_list = self.transformer(x) # L,N,D\n",
    "        \n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = x.reshape(b, c, h, w)\n",
    "        logit = self.linear_pred(x)\n",
    "        \n",
    "\n",
    "        return logit, attn_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cam_refined_list, keys, w, h = [perform_single_voc_cam(...)](WeCLIP/clip/clip_tool.py#L106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def perform_single_voc_cam(img_path, image, image_features, attn_weight_list, seg_attn, bg_text_features,\n",
    "                       fg_text_features, cam, mode='train', require_seg_trans=False):\n",
    "    \n",
    "    ## ---- Inout Processing ----\n",
    "    bg_text_features = bg_text_features.cuda()\n",
    "    fg_text_features = fg_text_features.cuda()\n",
    "    ori_image = Image.open(img_path)\n",
    "    ori_height, ori_width = np.asarray(ori_image).shape[:2]\n",
    "\n",
    "    ## ---- Label Processing ----\n",
    "    label_id_list = np.unique(ori_image)\n",
    "    label_id_list = (label_id_list - 1).tolist()\n",
    "    \n",
    "    # remove ignore labels\n",
    "    if 255 in label_id_list:\n",
    "        label_id_list.remove(255)\n",
    "    if 254 in label_id_list:\n",
    "        label_id_list.remove(254)\n",
    "\n",
    "    label_list = []\n",
    "    for lid in label_id_list:\n",
    "        label_list.append(new_class_names[int(lid)])\n",
    "    label_id_list = [int(lid) for lid in label_id_list]\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    h, w = image.shape[-2], image.shape[-1]\n",
    "\n",
    "    highres_cam_to_save = []\n",
    "    keys = []\n",
    "\n",
    "    cam_refined_list = []\n",
    "\n",
    "    ## ---- Feature preparation ----\n",
    "    bg_features_temp = bg_text_features.cuda()  # [bg_id_for_each_image[im_idx]].to(device_id)\n",
    "    fg_features_temp = fg_text_features[label_id_list].cuda()\n",
    "    text_features_temp = torch.cat([fg_features_temp, bg_features_temp], dim=0)\n",
    "    input_tensor = [image_features, text_features_temp.cuda(), h, w]\n",
    "\n",
    "\n",
    "    ## ---- CAM Refinement ----\n",
    "    for idx, label in enumerate(label_list):\n",
    "\n",
    "        # ---- generates grayscale CAM using CLIP features ----\n",
    "        label_index = new_class_names.index(label)\n",
    "        keys.append(label_index)\n",
    "        targets = [ClipOutputTarget(label_list.index(label))]\n",
    "        grayscale_cam, logits_per_image, attn_weight_last = cam(input_tensor=input_tensor,\n",
    "                                                                targets=targets,\n",
    "                                                                target_size=None)  # (ori_width, ori_height))\n",
    "\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "        grayscale_cam_highres = cv2.resize(grayscale_cam, (w, h))\n",
    "        highres_cam_to_save.append(torch.tensor(grayscale_cam_highres))\n",
    "\n",
    "        ## ---- Attention-based Refinement ----\n",
    "        if idx == 0:\n",
    "            if require_seg_trans == True:\n",
    "\n",
    "                ## ---- using difference thresholding ----\n",
    "                attn_weight = torch.cat([attn_weight_list, attn_weight_last], dim=0)\n",
    "                attn_weight = attn_weight[:, 1:, 1:][-6:] #-8\n",
    "                #NOTE: Hard-coded attention slice indices (-6 and -8)\n",
    "\n",
    "                # attn_diff = torch.abs(seg_attn - attn_weight)\n",
    "                attn_diff = seg_attn - attn_weight\n",
    "                attn_diff = torch.sum(attn_diff.flatten(1), dim=1)\n",
    "                diff_th = torch.mean(attn_diff)\n",
    "\n",
    "                attn_mask = torch.zeros_like(attn_diff)\n",
    "                attn_mask[attn_diff <= diff_th] = 1\n",
    "\n",
    "                attn_mask = attn_mask.reshape(-1, 1, 1)\n",
    "                attn_mask = attn_mask.expand_as(attn_weight)\n",
    "                attn_weight = torch.sum(attn_mask*attn_weight, dim=0) / (torch.sum(attn_mask, dim=0)+1e-5)\n",
    "\n",
    "                attn_weight = attn_weight.detach()\n",
    "                attn_weight = attn_weight * seg_attn.squeeze(0).detach()\n",
    "            else:\n",
    "                ## ---- using mean pooling ----\n",
    "                attn_weight = torch.cat([attn_weight_list, attn_weight_last], dim=0)\n",
    "                attn_weight = attn_weight[:, 1:, 1:][-8:]\n",
    "                attn_weight = torch.mean(attn_weight, dim=0)  # (1, hw, hw)\n",
    "                attn_weight = attn_weight.detach()\n",
    "            _trans_mat = compute_trans_mat(attn_weight)\n",
    "        _trans_mat = _trans_mat.float()\n",
    "\n",
    "        ## ---- compute bounding boxes from CAM ---- \n",
    "        \n",
    "        #NOTE: fixed threshold of 0.4\n",
    "        box, cnt = scoremap2bbox(scoremap=grayscale_cam, threshold=0.4, multi_contour_eval=True)\n",
    "        aff_mask = torch.zeros((grayscale_cam.shape[0], grayscale_cam.shape[1])).cuda()\n",
    "        for i_ in range(cnt):\n",
    "            x0_, y0_, x1_, y1_ = box[i_]\n",
    "            aff_mask[y0_:y1_, x0_:x1_] = 1\n",
    "\n",
    "        ## ---- create affinity mask ----\n",
    "        aff_mask = aff_mask.view(1, grayscale_cam.shape[0] * grayscale_cam.shape[1])\n",
    "        trans_mat = _trans_mat*aff_mask\n",
    "\n",
    "\n",
    "        ## ---- apply transformation matrix for final refinement ----\n",
    "        cam_to_refine = torch.FloatTensor(grayscale_cam).cuda()\n",
    "        cam_to_refine = cam_to_refine.view(-1, 1)\n",
    "\n",
    "        #NOTE: Resolution reduction by factor of 16 in final output\n",
    "        cam_refined = torch.matmul(trans_mat, cam_to_refine).reshape(h // 16, w // 16)\n",
    "        cam_refined_list.append(cam_refined)\n",
    "\n",
    "    if mode == 'train':\n",
    "        return cam_refined_list, keys, w, h\n",
    "    else:\n",
    "        return cam_refined_list, keys, ori_width, ori_height\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Improvements: \n",
    "- Make thresholds configurable\n",
    "- Consider batch processing support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bg_text_features = [zeroshot_classifier](WeCLIP/WeCLIP_model/model_attn_aff_voc.py#L34)(BACKGROUND_CATEGORY, ['a clean origami {}.'], self.encoder)\n",
    "\n",
    "fg_text_features = zeroshot_classifier(new_class_names, ['a clean origami {}.'], self.encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def zeroshot_classifier(classnames, templates, model):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in classnames:\n",
    "            texts = [template.format(classname) for template in templates] #format with class\n",
    "            texts = clip.tokenize(texts).cuda() #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True) ## normalize to unit length for cosine similarity\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### texts = [clip.tokenize(texts)](WeCLIP/clip/clip.py#L205).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Returns the tokenized representation of given input string(s)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : Union[str, List[str]]\n",
    "        An input string or a list of input strings to tokenize\n",
    "\n",
    "    context_length : int\n",
    "        The context length to use; all CLIP models use 77 as the context length\n",
    "\n",
    "    truncate: bool\n",
    "        Whether to truncate the text in case its encoding is longer than the context length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].\n",
    "    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    if packaging.version.parse(torch.__version__) < packaging.version.parse(\"1.8.0\"):\n",
    "        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "    else:\n",
    "        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class_embeddings = model.[encode_text(texts)](WeCLIP/clip/model.py#L392)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND (length, batch, dim)\n",
    "        x, attn_weight = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, attn_weight = self.[transformer](WeCLIP/clip/model.py#L218)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = self.[ln_final](WeCLIP/clip/model.py#L177)(x).type(self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grayscale_cam, logits_per_image, attn_weight_last = [cam(input_tensor, targets)](WeCLIP/pytorch_grad_cam/base_cam.py#L62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class GradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, use_cuda=False,\n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            use_cuda,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "\n",
    "        return np.mean(grads, axis=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class BaseCAM:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 target_layers: List[torch.nn.Module],\n",
    "                 use_cuda: bool = False,\n",
    "                 reshape_transform: Callable = None,\n",
    "                 compute_input_gradient: bool = False,\n",
    "                 uses_gradients: bool = True) -> None:\n",
    "        self.model = model.eval()\n",
    "        ...\n",
    "        self.activations_and_grads = ActivationsAndGradients(\n",
    "            self.model, target_layers, reshape_transform)\n",
    "    ...\n",
    "    def forward(self,\n",
    "                input_tensor: torch.Tensor,\n",
    "                targets: List[torch.nn.Module],\n",
    "                target_size,\n",
    "                eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        ## ---- Input Processing ----\n",
    "        if self.cuda:\n",
    "            input_tensor = input_tensor.cuda()\n",
    "        if self.compute_input_gradient:\n",
    "            input_tensor = torch.autograd.Variable(input_tensor,\n",
    "                                                   requires_grad=True)\n",
    "\n",
    "        W,H = self.get_target_width_height(input_tensor)\n",
    "        outputs = self.activations_and_grads(input_tensor,H,W)\n",
    "\n",
    "        ## ---- Target Management ----\n",
    "        '''\n",
    "        If no targets are provided, it automatically selects the highest scoring categories as targets. This handles both single inputs and lists of inputs.\n",
    "        '''\n",
    "        if targets is None:\n",
    "            if isinstance(input_tensor, list):\n",
    "                target_categories = np.argmax(outputs[0].cpu().data.numpy(), axis=-1)\n",
    "            else:\n",
    "                target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
    "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
    "\n",
    "        ## ---- Gradient Computation ----\n",
    "        if self.uses_gradients:\n",
    "            self.model.zero_grad()\n",
    "            if isinstance(input_tensor, list):\n",
    "                loss = sum([target(output[0]) for target, output in zip(targets, outputs)])\n",
    "            else:\n",
    "                loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "        # In most of the saliency attribution papers, the saliency is\n",
    "        # computed with a single target layer.\n",
    "        # Commonly it is the last convolutional layer.\n",
    "        # Here we support passing a list with multiple target layers.\n",
    "        # It will compute the saliency image for every image,\n",
    "        # and then aggregate them (with a default mean aggregation).\n",
    "        # This gives you more flexibility in case you just want to\n",
    "        # use all conv layers for example, all Batchnorm layers,\n",
    "        # or something else.\n",
    "        cam_per_layer = self.compute_cam_per_layer(input_tensor,\n",
    "                                                   targets,\n",
    "                                                   target_size,\n",
    "                                                   eigen_smooth)\n",
    "        if isinstance(input_tensor, list):\n",
    "            return self.aggregate_multi_layers(cam_per_layer), outputs[0], outputs[1]\n",
    "        else:\n",
    "            return self.aggregate_multi_layers(cam_per_layer), outputs\n",
    "\n",
    "    ...\n",
    "    # cam_per_layer = self.compute_cam_per_layer(...)\n",
    "    def compute_cam_per_layer(\n",
    "                self,\n",
    "                input_tensor: torch.Tensor,\n",
    "                targets: List[torch.nn.Module],\n",
    "                target_size,\n",
    "                eigen_smooth: bool) -> np.ndarray:\n",
    "\n",
    "            ## ---- Data Preparation ----\n",
    "            activations_list = [a.cpu().data.numpy()\n",
    "                                for a in self.activations_and_grads.activations]\n",
    "            grads_list = [g.cpu().data.numpy()\n",
    "                        for g in self.activations_and_grads.gradients]\n",
    "\n",
    "            cam_per_target_layer = []\n",
    "\n",
    "            ## ---- Loop over the saliency image from every layer ----\n",
    "            for i in range(len(self.target_layers)):\n",
    "                target_layer = self.target_layers[i]\n",
    "                layer_activations = None\n",
    "                layer_grads = None\n",
    "                if i < len(activations_list):\n",
    "                    layer_activations = activations_list[i]\n",
    "                if i < len(grads_list):\n",
    "                    layer_grads = grads_list[i]\n",
    "\n",
    "                cam = self.get_cam_image(input_tensor,\n",
    "                                        target_layer,\n",
    "                                        targets,\n",
    "                                        layer_activations,\n",
    "                                        layer_grads,\n",
    "                                        eigen_smooth)\n",
    "                # apply ReLU, 32-bit precision                        \n",
    "                cam = np.maximum(cam, 0).astype(np.float32)#float16->32\n",
    "                # scale to target size\n",
    "                scaled = scale_cam_image(cam, target_size)\n",
    "                cam_per_target_layer.append(scaled[:, None, :])\n",
    "\n",
    "            return cam_per_target_layer\n",
    "\n",
    "    # cam = self.get_cam_image(...)\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor: torch.Tensor,\n",
    "                      target_layer: torch.nn.Module,\n",
    "                      targets: List[torch.nn.Module],\n",
    "                      activations: torch.Tensor,\n",
    "                      grads: torch.Tensor,\n",
    "                      eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        weights = self.get_cam_weights(input_tensor,\n",
    "                                       target_layer,\n",
    "                                       targets,\n",
    "                                       activations,\n",
    "                                       grads)\n",
    "        weighted_activations = weights[:, :, None, None] * activations\n",
    "        if eigen_smooth:\n",
    "            cam = get_2d_projection(weighted_activations)\n",
    "        else:\n",
    "            cam = weighted_activations.sum(axis=1)\n",
    "        return cam\n",
    "\n",
    "    # weights = self.get_cam_weights(...)\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor: torch.Tensor,\n",
    "                        target_layers: List[torch.nn.Module],\n",
    "                        targets: List[torch.nn.Module],\n",
    "                        activations: torch.Tensor,\n",
    "                        grads: torch.Tensor) -> np.ndarray:\n",
    "        \n",
    "        \"\"\" Get a vector of weights for every channel in the target layer.\n",
    "        Methods that return weights channels,\n",
    "        will typically need to only implement this function. \"\"\"\n",
    "\n",
    "        raise Exception(\"Not Implemented\")      #BUG: WHAT?!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cam_dict = [generate_cam_label(cam_refined_list, keys, w, h)](WeCLIP/clip/clip_tool.py#L202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def generate_cam_label(cam_refined_list, keys, w, h):\n",
    "    refined_cam_to_save = []\n",
    "    refined_cam_all_scales = []\n",
    "    for cam_refined in cam_refined_list:\n",
    "        cam_refined = cam_refined.cpu().numpy().astype(np.float32)\n",
    "        cam_refined_highres = scale_cam_image([cam_refined], (w, h))[0]\n",
    "        refined_cam_to_save.append(torch.tensor(cam_refined_highres))\n",
    "\n",
    "    keys = torch.tensor(keys)\n",
    "\n",
    "    refined_cam_all_scales.append(torch.stack(refined_cam_to_save,dim=0))\n",
    "\n",
    "    refined_cam_all_scales = refined_cam_all_scales[0]\n",
    "    \n",
    "    return {'keys': keys.numpy(), 'refined_cam':refined_cam_all_scales}\n",
    "\n",
    "## cam_refined_highres = scale_cam_image([cam_refined], (w, h))[0]\n",
    "# from WeCLIP/pytorch_grad_cam/utils/image.py#L51\n",
    "def scale_cam_image(cam, target_size=None):\n",
    "    result = []\n",
    "    for img in cam:\n",
    "        img = img - np.min(img)\n",
    "        img = img / (1e-7 + np.max(img))\n",
    "        if target_size is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "        result.append(img)\n",
    "    result = np.float32(result)\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cam_labels = [_refine_cams(self.par, img[i], cams, valid_key)](WeCLIP/WeCLIP_model/model_attn_aff_voc.py#L49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def _refine_cams(ref_mod, images, cams, valid_key):\n",
    "    images = images.unsqueeze(0)\n",
    "    cams = cams.unsqueeze(0)\n",
    "\n",
    "    refined_cams = ref_mod(images.float(), cams.float())\n",
    "    refined_label = refined_cams.argmax(dim=1)\n",
    "    refined_label = valid_key[refined_label]\n",
    "\n",
    "    return refined_label.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### refined_cams = [ref_mod(images.float(), cams.float())](WeCLIP/WeCLIP_model/PAR.py#L64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "class PAR(nn.Module):\n",
    "\n",
    "    def __init__(self, dilations, num_iter,):\n",
    "        super().__init__()\n",
    "        self.dilations = dilations\n",
    "        self.num_iter = num_iter\n",
    "        kernel = get_kernel()\n",
    "        self.register_buffer('kernel', kernel)\n",
    "        self.pos = self.get_pos()\n",
    "        self.dim = 2\n",
    "        self.w1 = 0.3\n",
    "        self.w2 = 0.01\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    def forward(self, imgs, masks):\n",
    "\n",
    "        # masks = F.interpolate(masks, size=imgs.size()[-2:], mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        ## ensures images and masks are at the same resolution using bilinear interpolation\n",
    "        imgs = F.interpolate(imgs, size=masks.size()[-2:], mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        b, c, h, w = imgs.shape\n",
    "        ## ---- Neighbor and Position Processing ----\n",
    "        _imgs = self.get_dilated_neighbors(imgs)\n",
    "        _pos = self.pos.to(_imgs.device)\n",
    "\n",
    "        _imgs_rep = imgs.unsqueeze(self.dim).repeat(1,1,_imgs.shape[self.dim],1,1) # (b, c, h*w, h, w), repeat for each neighbor position\n",
    "        _pos_rep = _pos.repeat(b, 1, 1, h, w) # (b, 8, h*w, h, w), repeat for each image\n",
    "\n",
    "        ## ---- Affinity Computation ----\n",
    "        ### --- Appeareance-based ---\n",
    "        _imgs_abs = torch.abs(_imgs - _imgs_rep)\n",
    "        _imgs_std = torch.std(_imgs, dim=self.dim, keepdim=True)\n",
    "        _pos_std = torch.std(_pos_rep, dim=self.dim, keepdim=True)\n",
    "\n",
    "        aff = -(_imgs_abs / (_imgs_std + 1e-8) / self.w1)**2\n",
    "        aff = aff.mean(dim=1, keepdim=True)\n",
    "\n",
    " \n",
    "        ### --- Position-based ---\n",
    "        pos_aff = -(_pos_rep / (_pos_std + 1e-8) / self.w1)**2\n",
    "        #pos_aff = pos_aff.mean(dim=1, keepdim=True)\n",
    "\n",
    "        #NOTE: 1e-8 term prevents division by zero but might need adjustment for different scales\n",
    "        \n",
    "        aff = F.softmax(aff, dim=2) + self.w2 * F.softmax(pos_aff, dim=2)\n",
    "        #NOTE: weights w1 and w2 critically affect the balance between appearance and position affinities\n",
    "\n",
    "        ## ---- Iterative Refinement ----\n",
    "        for _ in range(self.num_iter):\n",
    "            _masks = self.get_dilated_neighbors(masks)\n",
    "            masks = (_masks * aff).sum(2)\n",
    "\n",
    "        #TODO: Potential Improvement: add early stopping criteria \n",
    "        '''\n",
    "        prev_mask = None\n",
    "        for _ in range(self.num_iter):\n",
    "            prev_mask = masks.clone()\n",
    "            _masks = self.get_dilated_neighbors(masks)\n",
    "            masks = (_masks * aff).sum(2)\n",
    "            if torch.abs(masks - prev_mask).max() < 1e-6:\n",
    "                break\n",
    "        '''\n",
    "        return masks\n",
    "\n",
    "    # _imgs = self.get_dilated_neighbors(imgs)\n",
    "    def get_dilated_neighbors(self, x):\n",
    "        \"\"\"\n",
    "        Creates a dilated convolution pattern to gather neighboring pixel information.\n",
    "        Crucial for understanding local image context at different scales.\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "        x_aff = []\n",
    "        for d in self.dilations:\n",
    "            _x_pad = F.pad(x, [d]*4, mode='replicate', value=0)\n",
    "            _x_pad = _x_pad.reshape(b*c, -1, _x_pad.shape[-2], _x_pad.shape[-1])\n",
    "            _x = F.conv2d(_x_pad, self.kernel, dilation=d).view(b, c, -1, h, w)\n",
    "            x_aff.append(_x)\n",
    " \n",
    "        return torch.cat(x_aff, dim=2)\n",
    "    \n",
    "    # _pos = self.pos.to(_imgs.device)\n",
    "    # self.pos = self.get_pos()\n",
    "    def get_pos(self):\n",
    "        pos_xy = []\n",
    "\n",
    "        ker = torch.ones(1, 1, 8, 1, 1)\n",
    "        '''\n",
    "        initializes a 5D tensor representing a kernel with 8 positions. The dimensions represent:\n",
    "            Batch size (1)\n",
    "            Channel (1)\n",
    "            Positions (8)\n",
    "            Height (1)\n",
    "            Width (1)\n",
    "        '''\n",
    "        # Special Position Weighting, geometric pattern for relative spatial relationships\n",
    "        ker[0, 0, 0, 0, 0] = np.sqrt(2)\n",
    "        ker[0, 0, 2, 0, 0] = np.sqrt(2)\n",
    "        ker[0, 0, 5, 0, 0] = np.sqrt(2)\n",
    "        ker[0, 0, 7, 0, 0] = np.sqrt(2)\n",
    "        \n",
    "        # scale by different dilations and concatenate along dim=2 (position)\n",
    "        for d in self.dilations:\n",
    "            pos_xy.append(ker*d)\n",
    "        return torch.cat(pos_xy, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attn_mask = [get_mask_by_radius](WeCLIP/scripts/dist_clip_voc.py#L116)(h=mask_size, w=mask_size, radius=args.radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def get_mask_by_radius(h=20, w=20, radius=8):\n",
    "    '''\n",
    "    creates a square binary matrix of size (h*w, h*w) where each row/column represents a pixel position, and 1's indicate connected pixels within the specified radius.\n",
    "    '''\n",
    "    # create square binary, each pixel : position\n",
    "    hw = h * w\n",
    "    mask  = np.zeros((hw, hw))\n",
    "\n",
    "    for i in range(hw):\n",
    "        _h = i // w\n",
    "        _w = i % w\n",
    "        \n",
    "        # neighbors definition\n",
    "        _h0 = max(0, _h - radius)\n",
    "        _h1 = min(h, _h + radius+1)\n",
    "        _w0 = max(0, _w - radius)\n",
    "        _w1 = min(w, _w + radius+1)\n",
    "\n",
    "        for i1 in range(_h0, _h1):\n",
    "            for i2 in range(_w0, _w1):\n",
    "                _i2 = i1 * w + i2   # Convert 2D coordinates to linear index\n",
    "                mask[i, _i2] = 1    # forward connection\n",
    "                mask[_i2, i] = 1    # backward connection   #NOTE: symmetric matrix\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aff_label = [cams_to_affinity_label](WeCLIP/utils/camutils.py#L226)(fts_cam, mask=attn_mask, ignore_index=cfg.dataset.ignore_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def cams_to_affinity_label(cam_label, mask=None, ignore_index=255):\n",
    "    \n",
    "    b,h,w = cam_label.shape\n",
    "    \n",
    "    ## Downsampling to reduce computational complexity with Neighbor interpolation\n",
    "    cam_label_resized = F.interpolate(cam_label.unsqueeze(1).type(torch.float32), size=[h//16, w//16], mode=\"nearest\")\n",
    "\n",
    "    # cam_label_resized = F.interpolate(cam_label.unsqueeze(1).type(torch.float32), size=[h//8, w//8], mode=\"nearest\")\n",
    "\n",
    "    _cam_label = cam_label_resized.reshape(b, 1, -1) # (b, 1, h*w)\n",
    "    _cam_label_rep = _cam_label.repeat([1, _cam_label.shape[-1], 1]) # (b, h*w, h*w) #NOTE: repeat to create comparison matrix\n",
    "    _cam_label_rep_t = _cam_label_rep.permute(0,2,1)\n",
    "    \n",
    "    ## Affinity: 1 if same class, 0 otherwise\n",
    "    aff_label = (_cam_label_rep == _cam_label_rep_t).type(torch.long)\n",
    "    #aff_label[(_cam_label_rep+_cam_label_rep_t) == 0] = ignore_index\n",
    "\n",
    "    ## Mask application\n",
    "    for i in range(b):\n",
    "\n",
    "        if mask is not None:\n",
    "            aff_label[i, mask==0] = ignore_index\n",
    "\n",
    "        aff_label[i, :, _cam_label_rep[i, 0, :]==ignore_index] = ignore_index\n",
    "        aff_label[i, _cam_label_rep[i, 0, :]==ignore_index, :] = ignore_index\n",
    "\n",
    "    return aff_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attn_loss, pos_count, neg_count = [get_aff_loss](WeCLIP/utils/losses.py#L11)(attn_pred, aff_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def get_aff_loss(inputs, targets):\n",
    "\n",
    "    pos_label = (targets == 1).type(torch.int16)    # positive\n",
    "    pos_count = pos_label.sum() + 1                 # prevent division by zero\n",
    "\n",
    "    neg_label = (targets == 0).type(torch.int16)    # negative\n",
    "    neg_count = neg_label.sum() + 1\n",
    "    #inputs = torch.sigmoid(input=inputs)\n",
    "\n",
    "    pos_loss = torch.sum(pos_label * (1 - inputs)) / pos_count\n",
    "    neg_loss = torch.sum(neg_label * (inputs)) / neg_count\n",
    "\n",
    "    return 0.5 * pos_loss + 0.5 * neg_loss, pos_count, neg_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seg_loss = [get_seg_loss](WeCLIP/utils/losses.py#L24)(segs, pseudo_label.type(torch.long), ignore_index=cfg.dataset.ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script True\n",
    "\n",
    "def get_seg_loss(pred, label, ignore_index=255):\n",
    "    \n",
    "    ## ---- Background ----\n",
    "    bg_label = label.clone()\n",
    "    bg_label[label!=0] = ignore_index\n",
    "    bg_loss = F.cross_entropy(pred, bg_label.type(torch.long), ignore_index=ignore_index)\n",
    "    \n",
    "    ## ---- Foreground ----\n",
    "    fg_label = label.clone()\n",
    "    fg_label[label==0] = ignore_index\n",
    "    fg_loss = F.cross_entropy(pred, fg_label.type(torch.long), ignore_index=ignore_index)\n",
    "\n",
    "    return (bg_loss + fg_loss) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss = 1 * seg_loss + 0.1*attn_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frozenCLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
